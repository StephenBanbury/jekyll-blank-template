<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-07-25T19:56:38+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Creative Reflective Journal</title><subtitle>My Creative Reflective Journal towards the Falmouth University Creative App Development Masters Degree.</subtitle><entry><title type="html">Streaming To And From Unity</title><link href="http://localhost:4000/2020/07/25/streaming-into-unity.html" rel="alternate" type="text/html" title="Streaming To And From Unity" /><published>2020-07-25T15:00:00+01:00</published><updated>2020-07-25T15:00:00+01:00</updated><id>http://localhost:4000/2020/07/25/streaming-into-unity</id><content type="html" xml:base="http://localhost:4000/2020/07/25/streaming-into-unity.html">&lt;h1 id=&quot;the-requirement&quot;&gt;The requirement&lt;/h1&gt;

&lt;p&gt;I need to find a way to take data from a device and stream it into my Unity scene.&lt;/p&gt;

&lt;p&gt;An example would be taking video from a webcam or a mobile phone camera, creating a data stream to which a Unity script can subscribe and then display the streamed video onto a texture within the scene.&lt;/p&gt;

&lt;p&gt;Another, perhaps simpler example, would be to take numeric 2D positional data, manipulated on the screen of the extertanl device, and stream that to Unity.  The data can then be used to control the position of an object within the scene.&lt;/p&gt;

&lt;p&gt;Audio would be another, allowing a stream of audio data from the external device’s microphone to enter the Unity scene and be heard by the players via the audio mixer.&lt;/p&gt;

&lt;h1 id=&quot;thinking-about-options&quot;&gt;Thinking about options&lt;/h1&gt;

&lt;p&gt;Would it be best, or feasible, to create a room in the Unity project that can access the device’s camera resources, and then to create a build for each platform - i.e. Android (can be the same build as for the Oculus quest), iOs and Windows.&lt;/p&gt;

&lt;p&gt;Or could the unity project be cloud-hosted, allowing users to connect via the internet. The cloud app would need to be able to access the device’s resources across the internet, much like Zoom or Skype etc.&lt;/p&gt;

&lt;p&gt;An alternative may be to create a web app that can run in a browser - perhaps raw JavaScript or an Angular/Vue/React etc. app, which can access the device’s resources and stream via a socket to a server. The Unity project can then connect to a socket on the server and subscribe to the datastream.&lt;/p&gt;

&lt;p&gt;Other than these options we could be looking at either a cross-platform option, like Flutter or Ionic, or multiple Native apps.&lt;/p&gt;

&lt;h1 id=&quot;random-research&quot;&gt;Random research&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://stackoverflow.com/questions/37100900/unity-processing-data-stream-from-socket&quot;&gt;StackOverflow&lt;/a&gt;&lt;/strong&gt;: Unity Processing Data Stream from Socket.  This is an interesting question asked on StackOverflow and it leads to a &lt;strong&gt;potential solution&lt;/strong&gt; for setting up a TCP client in Unity: &lt;strong&gt;&lt;a href=&quot;https://stackoverflow.com/questions/36526332/simple-socket-server-in-unity/36526634#36526634&quot;&gt;Simple socket server in Unity&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://stackoverflow.com/questions/46564222/how-to-send-and-receive-tcp-messages-while-streaming-video-unity-and-socket-ne&quot;&gt;Another on StackOverflow&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://stackoverflow.com/questions/42717713/unity-live-video-streaming/42727918#42727918&quot;&gt;And another…&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://stackoverflow.com/questions/17719541/writing-and-reading-using-socket&quot;&gt;… and guess what…&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mozilla:&lt;/strong&gt; &lt;strong&gt;&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API&quot;&gt;The WebSocket API (WebSockets)&lt;/a&gt;&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API/Writing_WebSocket_server&quot;&gt;Writing a WebSocket server in C#&lt;/a&gt;&lt;/strong&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another possible option could be to re-use the code already implemented in my &lt;strong&gt;&lt;a href=&quot;/2020/07/11/client-server-tutorial.html&quot;&gt;C# TCP/UDP multiplayer experiment&lt;/a&gt;&lt;/strong&gt;.  As it is basically about sending and receiving data between clients via a server, in a sense it is already all in place. I’m tempted to combine this with what I could learn from the above Mozilla WebSocket documentation and to build the solution from the ground up.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://webrtc.org/&quot;&gt;WebRTC&lt;/a&gt;&lt;/strong&gt;: I keep coming back to this - it’s potentially the all-encompassing solution, covering browser, native, Unity - both client and server, both directions possible (or so it would seem).
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;/2020/07/02/webrtc.html&quot;&gt;My earlier post on WebRTC&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://microsoft.github.io/MixedReality-WebRTC/manual/gettingstarted.html&quot;&gt;Microsoft’s WebRTC project&lt;/a&gt;&lt;/strong&gt;, including a library for &lt;strong&gt;&lt;a href=&quot;https://microsoft.github.io/MixedReality-WebRTC/manual/unity/unity-integration.html&quot;&gt;Unity&lt;/a&gt;&lt;/strong&gt; and a &lt;strong&gt;&lt;a href=&quot;https://microsoft.github.io/MixedReality-WebRTC/manual/unity/helloworld-unity.html&quot;&gt;Unity tutorial&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://docs.unity3d.com/Packages/com.unity.webrtc@2.0/manual/index.html&quot;&gt;Unity documentation&lt;/a&gt;&lt;/strong&gt;: includes links to a &lt;strong&gt;&lt;em&gt;Tutorial&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;Reference&lt;/em&gt;&lt;/strong&gt; for &lt;strong&gt;&lt;em&gt;Video&lt;/em&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;em&gt;Audio&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;Data&lt;/em&gt;&lt;/strong&gt; streaming.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">The requirement</summary></entry><entry><title type="html">Taking Stock - Sprint 1 Retrospective</title><link href="http://localhost:4000/2020/07/25/taking-stock.html" rel="alternate" type="text/html" title="Taking Stock - Sprint 1 Retrospective" /><published>2020-07-25T10:00:00+01:00</published><updated>2020-07-25T10:00:00+01:00</updated><id>http://localhost:4000/2020/07/25/taking-stock</id><content type="html" xml:base="http://localhost:4000/2020/07/25/taking-stock.html">&lt;h1 id=&quot;as-sprint-1-comes-to-a-close-what-have-i-achieved&quot;&gt;As Sprint 1 comes to a close, what have I achieved?&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;I have looked at the potential of &lt;strong&gt;&lt;a href=&quot;https://docs.unity3d.com/Manual/XR.html&quot;&gt;Unity XR plug-in framework&lt;/a&gt;&lt;/strong&gt; as a replacement for Oculus Integration, which, accoring to Unity, will be deprecated in a future release. For now, I have decided to continue using Oculus Integration as it is a framework I with which I have experience, is mature, well-documented, and is still being developed by Oculus with the very interesting inclusing of hand-tracking - potentially something that will be experimented with during this project.&lt;/li&gt;
  &lt;li&gt;I have looked at the potential of &lt;strong&gt;&lt;a href=&quot;https://docs.unity3d.com/Manual/XR.html&quot;&gt;Unity XR plug-in framework&lt;/a&gt;&lt;/strong&gt; as a replacement for Oculus Integration, which, accoring to Unity, will be deprecated in &lt;em&gt;“a future release”&lt;/em&gt;. However - &lt;strong&gt;STOP PRESS&lt;/strong&gt; - I have now installed the latest version of Unity - 2020.1 - in which it appears that the old XR integration &lt;strong&gt;has&lt;/strong&gt; been deprecated. However, it also appears that Oculus Integration &lt;strong&gt;has not&lt;/strong&gt; been replaced, but actually fits in with the new Unity framework. In fact, it appears to be very easy to integrate - I just needed to enable &lt;strong&gt;XR Plug-in Management&lt;/strong&gt; and it appears to work seamlessly with Oculus Integration! I do not need to import any XR package, as was required when using the XR plug-in in my earlier experiments. The package manager appears to be much tidyer than it was too.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Old XR Settings have now disappeared&lt;/strong&gt;
&lt;img src=&quot;\images\GAM750\unity-xr-deprecated-2020.jpg&quot; alt=&quot;Unity 2020.1 changes to XR Integration and Package Manager&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;New XR Plug-in Management&lt;/strong&gt;
&lt;img src=&quot;\images\GAM750\unity-xr-plugin-1.jpg&quot; alt=&quot;Unity 2020.1 changes to XR Integration and Package Manager&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Old vs New Package Manager&lt;/strong&gt;
&lt;img src=&quot;\images\GAM750\unity-xr-plugin-2.jpg&quot; alt=&quot;Unity 2020.1 changes to XR Integration and Package Manager&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I have tested several potential options for creating a multiplayer environment. This has enabled me to make an informed decision to use &lt;strong&gt;&lt;a href=&quot;https://normcore.io/&quot;&gt;Normcore&lt;/a&gt;&lt;/strong&gt;. Although it is relatively new and is still in beta, it has the potential to simplify the process of syncing objects and events within multiplayer environments.&lt;/li&gt;
  &lt;li&gt;I have created a player character/avatar that syncs accross all clients, including transforms and voice captured via the microphone.&lt;/li&gt;
  &lt;li&gt;I have created game objects whose transforms are synced across clients.&lt;/li&gt;
  &lt;li&gt;I have learned how to create custom components that can sync across clients, such as mesh colour.&lt;/li&gt;
  &lt;li&gt;I have created a custom component that syncs an abstract, numerical variable value across clients.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;what-do-i-still-need-to-do&quot;&gt;What do I still need to do?&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Learn how to stream data, images, video and sound from external sources into the environment where it can be displayed in realtime.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;future-research-once-all-this-has-been-achieved&quot;&gt;Future research, once all this has been achieved&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;How to separate incoming video streams for individual performers so as to place them discretely onto individually-designated displays within the VR environment.&lt;/li&gt;
  &lt;li&gt;How to separate and route individual voice data from specific players within the VR environment to
    &lt;ol&gt;
      &lt;li&gt;other players within and sharing the VR environment&lt;/li&gt;
      &lt;li&gt;performers external to the VR environment.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;How to stream video capture from within the VR environment out to external devices. This will allow an external ‘audience’ to live-view the environment via, for example, Google Cardboard devices or simply on mobile device screens.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I will add more items here as I think of them…&lt;/p&gt;</content><author><name></name></author><summary type="html">As Sprint 1 comes to a close, what have I achieved?</summary></entry><entry><title type="html">Syncing Data In Normcore 2</title><link href="http://localhost:4000/2020/07/24/normcore-7.html" rel="alternate" type="text/html" title="Syncing Data In Normcore 2" /><published>2020-07-24T00:00:00+01:00</published><updated>2020-07-24T00:00:00+01:00</updated><id>http://localhost:4000/2020/07/24/normcore-7</id><content type="html" xml:base="http://localhost:4000/2020/07/24/normcore-7.html">&lt;h1 id=&quot;success&quot;&gt;Success!&lt;/h1&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\normcore-3.1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;The &lt;strong&gt;x&lt;/strong&gt; coordinate value from the player’s hand when within the blue oblong collider is displayed while also being passed to its model. Any changes within the model are detected by the equivalent model belonging to the other clients, the respective component is informed and the display is updated.&lt;/p&gt;

&lt;p&gt;The wonderful thing is that Normcore autogenates much of the code required to keep the models in sync. It’s then relatively straightforward to use this for your own means. I converted what I’d done to sync mesh colour as it is changed via the colour picker, but amended to sync the float value. &lt;strong&gt;The idea being that this value can be then translated into anything&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;And to celebrate..&lt;/strong&gt; we’ll throw some objects around.&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\normcore3.2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;If only I could play with a &lt;strong&gt;real person&lt;/strong&gt; rather than myself!&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\normcore3.3.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;</content><author><name></name></author><summary type="html">Success!</summary></entry><entry><title type="html">Syncing Data In Normcore 1</title><link href="http://localhost:4000/2020/07/23/normcore-6.html" rel="alternate" type="text/html" title="Syncing Data In Normcore 1" /><published>2020-07-23T00:00:00+01:00</published><updated>2020-07-23T00:00:00+01:00</updated><id>http://localhost:4000/2020/07/23/normcore-6</id><content type="html" xml:base="http://localhost:4000/2020/07/23/normcore-6.html">&lt;p&gt;I’m setting out here to try to solve the problem of syncing abstract data between two instances in a multiplayer environment.&lt;/p&gt;

&lt;p&gt;The Normcore pattern of keeping gameobjects in sync with a model held in the datastore should, in theory, also work for simple values. Here I’m attempting to find out how to do this.&lt;/p&gt;

&lt;p&gt;I’m looking in particular at the tutorial &lt;strong&gt;&lt;a href=&quot;https://normcore.io/documentation/guides/synchronizing-your-own-data.html&quot;&gt;Synchronizing your own data with custom Realtime Components&lt;/a&gt;&lt;/strong&gt; here for some guidance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;You see the problem…&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\normcore-sync-issue-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;blockquote&gt;
  &lt;p&gt;A RealtimeComponent keeps a game object in sync with its corresponding model in the datastore. When the game object changes, it updates the model, and when the model changes, it updates the game object to match.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I have already followed the &lt;strong&gt;&lt;a href=&quot;/2020/07/16/normcore-3.html&quot;&gt;Normcore documentation-tutorial&lt;/a&gt;&lt;/strong&gt; on changing the colour of an obect, and I hope to follow a similar pattern here. However, instead of colour, I will use an abstract numerical value. This, I hope, can be the basis of transforming any kind of data, returned from any kind of action, into any other kind of action. I may be over-egging the problem here - it may be simpler than I imagine. But this should at least be a valuable learning excercise!&lt;/p&gt;</content><author><name></name></author><summary type="html">I’m setting out here to try to solve the problem of syncing abstract data between two instances in a multiplayer environment.</summary></entry><entry><title type="html">Multiplayer Made Simple</title><link href="http://localhost:4000/2020/07/22/normcore-5.html" rel="alternate" type="text/html" title="Multiplayer Made Simple" /><published>2020-07-22T00:00:00+01:00</published><updated>2020-07-22T00:00:00+01:00</updated><id>http://localhost:4000/2020/07/22/normcore-5</id><content type="html" xml:base="http://localhost:4000/2020/07/22/normcore-5.html">&lt;p&gt;Building a simple multiplayer is made easy in Normcore.&lt;/p&gt;

&lt;p&gt;This example uses the Oculus OVRPlayerController, but it could just as easily use any other solution, such as the Unity XR integration.&lt;/p&gt;

&lt;p&gt;A Normcore Realtime VR + Player is included in the scene and the OVRPlayerController is wired in as the Local Player variable. That’s all that is required to instantiate the avatar in each multiplayer instance!&lt;/p&gt;

&lt;p&gt;The grabbable object script is also attached to the Realtime VR + Player, and Realtime View and Realtime Transform scripts are added to the grabbable object, along with a simple script that requests ownership of the object - via its Realtime Transform component - when it is grabbed by one of the players. If the Realtime View’s ‘Owned by Creating Client’ is checked then the other player cannot gain ownership.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  private void Update()
{
    if (gameObject.GetComponent&amp;lt;OVRGrabbable&amp;gt;().isGrabbed)
    {
        //potentially clear ownership first - if owned
        _realtimeTransform.RequestOwnership();
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;two-players-one-headset&quot;&gt;Two players, one headset!&lt;/h1&gt;

&lt;p&gt;Testing is a bit of a problem of course, with only one Oculus Quest (and only one developer!), both players cannot be independently moved at the same time. But it is possible to see the other player and use one’s imagination! Unfortunately, without two players it is not possible to try out throwing and catching - that would be great fun!&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\normcore-2.1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;The sound of the player’s voice is also picked up enabling lipsyncing. I will be looking at ways to stream the voice to other players.&lt;/p&gt;</content><author><name></name></author><summary type="html">Building a simple multiplayer is made easy in Normcore.</summary></entry><entry><title type="html">Normcore 2 Preview</title><link href="http://localhost:4000/2020/07/17/normcore-4.html" rel="alternate" type="text/html" title="Normcore 2 Preview" /><published>2020-07-17T00:00:00+01:00</published><updated>2020-07-17T00:00:00+01:00</updated><id>http://localhost:4000/2020/07/17/normcore-4</id><content type="html" xml:base="http://localhost:4000/2020/07/17/normcore-4.html">&lt;p&gt;I contacted Max Weisel of the Normcore team, via their Discord server, with a question, as I was having trouble getting objects to render in the other client instance - I could just see a horizon. Max responded with the solution (ensure the build is 64bit) in a matter of minutes - very impressive! I mentioned how impressed I am with Normcore and he suggested I contact Nick Savarese - another Normcore team member - to be given access to the Normcore 2 Preview. Apparently, the transitions are significantly smoother.&lt;/p&gt;

&lt;p&gt;This I did, and Nick sent me a message followed by emails with attached Unity projects with the preview versions pre-installed. It’s great to be at the cutting edge!&lt;/p&gt;

&lt;h1 id=&quot;normcore-200-preview-1&quot;&gt;Normcore 2.0.0 Preview 1&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Heyo,&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Thanks for your interest in trying out Normcore 2.0! Most of you are on Normcore 1.0, so we’ll be rolling out Normcore 2.0 features in stages in order to make upgrading as easy as possible.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;The first preview build is almost identical to the Normcore 1.0 API, however it features a completely new transport system and backend. Latency will be even lower and this backend should be much more stable. We’re currently only running the 2.0 backend on our US clusters. We’ll be bringing all of our regions online in about two weeks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;If you’re starting out on a fresh project with Normcore 2.0, you’ll want to import the Unity package without importing the Examples folder first. Then once the package is imported, you can reimport it again if you’d like to bring the examples into the project.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Upgrading from Normcore 1.0
Ensure your project currently compiles, leave all existing Normcore files in place, and backup your project! Then follow these instructions exactly:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;Import the unitypackage and wait for it to compile once.&lt;/li&gt;
    &lt;li&gt;After the first compilation, Normcore will be added in Package Manager. Unity will recompile again, and you’ll have a ton of duplicate symbol errors.&lt;/li&gt;
    &lt;li&gt;Delete the Realtime folder under Normal, and your project will start compiling again.&lt;/li&gt;
    &lt;li&gt;All that’s left is to fix any scene or prefabs that reference Normcore components. Go to Window &amp;gt; Normcore &amp;gt; Migrate and wait a few seconds for it to complete.&lt;/li&gt;
    &lt;li&gt;Once migration finishes, you’re good to go!&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Feel free to email us if you have any questions. we’ve also created a #normcore2-preview channel on our discord!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Stay Normal!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;normcore-200-preview-8&quot;&gt;Normcore 2.0.0 Preview 8&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Hey everyone,&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;We’ve now deployed the breaking backend change and have pushed an updated Unity package.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;This update also moved a few files around in the Examples folder. If you get an error while updating, please delete the Normal folder, and import the latest Unity package attached here.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;If you’ve previously migrated RealtimeAvatar/RealtimeAvatarManager scripts in your scenes and prefabs, we’ve moved them back to source files so people can copy them easily. You’ll need to manually migrate them back, or revert to copies of your scenes / prefabs before the migration and run the migration script again.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;If you’re starting out on a fresh project with Normcore 2.0, you’ll want to import the Unity package without importing the Examples folder first. Then once the package is imported, you can reimport it again if you’d like to bring the examples into the project.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;As always, feel free to email us if you have any questions or hit us up on the #normcore2-preview channel on our discord!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Best,
Nick&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name></name></author><summary type="html">I contacted Max Weisel of the Normcore team, via their Discord server, with a question, as I was having trouble getting objects to render in the other client instance - I could just see a horizon. Max responded with the solution (ensure the build is 64bit) in a matter of minutes - very impressive! I mentioned how impressed I am with Normcore and he suggested I contact Nick Savarese - another Normcore team member - to be given access to the Normcore 2 Preview. Apparently, the transitions are significantly smoother.</summary></entry><entry><title type="html">Normcore - Feeling Good</title><link href="http://localhost:4000/2020/07/16/normcore-2.html" rel="alternate" type="text/html" title="Normcore - Feeling Good" /><published>2020-07-16T00:00:00+01:00</published><updated>2020-07-16T00:00:00+01:00</updated><id>http://localhost:4000/2020/07/16/normcore-2</id><content type="html" xml:base="http://localhost:4000/2020/07/16/normcore-2.html">&lt;h1 id=&quot;feeling-good-about-normcore&quot;&gt;Feeling good about Normcore&lt;/h1&gt;

&lt;p&gt;Normcore has everything that I imagined I would be spending a significant amount of time building, but packaged up and ready to use. For this reason alone, it is extremely attractive as a means to building a multiplayer environment. I have so far begin exploring the idea of building my own server-client in C# and using TCP and UDP, Photon PUN 2 as an out-of-the-box solution and Mirror as a Unity-integrated and relatively simple way of wiring multiplayer clients together.&lt;/p&gt;

&lt;p&gt;The one thing that sets Normcore apart is that, simply put, it just works - or so it would seem. Particularly  with regard to VR. As a VR mutliplayer developer, the basics have already been done for you. The disadvantage, then is that, if you really do want to get to understand the mechanics of multiplayer networking, of how physics are translated from one client to another etc., these are mainly done by Normcore and it’s less likely you will learn from a deap, low-level perspective.&lt;/p&gt;

&lt;p&gt;Even so, it is still necessary to understand the model that is used by Normcore, and to understand it well.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\images\GAM750\normcore-flow-1.jpg&quot; alt=&quot;Normcore data flow&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A RealtimeComponent keeps a game object in sync with its corresponding model in the datastore. When the game object changes, it updates the model, and when the model changes, it updates the game object to match. This means that in the diagram above, when Player 1 moves a game object, RealtimeTransform can set the new position on its model in the datastore. When Player 2 gets a notification that the model changed, it can update the position of the matching game object in its scene&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name></name></author><summary type="html">Feeling good about Normcore</summary></entry><entry><title type="html">Exploring Normcore</title><link href="http://localhost:4000/2020/07/16/normcore-3.html" rel="alternate" type="text/html" title="Exploring Normcore" /><published>2020-07-16T00:00:00+01:00</published><updated>2020-07-16T00:00:00+01:00</updated><id>http://localhost:4000/2020/07/16/normcore-3</id><content type="html" xml:base="http://localhost:4000/2020/07/16/normcore-3.html">&lt;h1 id=&quot;exploring-normcore-via-tutorials&quot;&gt;Exploring Normcore via tutorials&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;moving body&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\normcore-sync-movement.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;avatar&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\normcore-sync-avatar.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;syncing colour&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\normcore-sync-colour.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;</content><author><name></name></author><summary type="html">Exploring Normcore via tutorials</summary></entry><entry><title type="html">The Project - Not Near Enough</title><link href="http://localhost:4000/2020/07/15/not-near-enough.html" rel="alternate" type="text/html" title="The Project - Not Near Enough" /><published>2020-07-15T00:00:00+01:00</published><updated>2020-07-15T00:00:00+01:00</updated><id>http://localhost:4000/2020/07/15/not-near-enough</id><content type="html" xml:base="http://localhost:4000/2020/07/15/not-near-enough.html">&lt;p&gt;&lt;strong&gt;Not Near Enough&lt;/strong&gt; is a &lt;strong&gt;&lt;a href=&quot;https://www.storyfutures.com/academy&quot;&gt;StoryFutures Academy&lt;/a&gt;&lt;/strong&gt;-funded project at Falmouth University exploring VR theatre spaces.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The brief for this project is to create an adaptable and interactive virtual theatre space where
actors and audience meet. The same space would also function as a teaching and rehearsal
space for Falmouth students and staff.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;During the past week I put together an expression of interest, resulting in an invitation from the team leaders to discuss the possibility of my working on the project. This took place on Monday, and yesterday (Tuesday) I was invited to join the project as &lt;strong&gt;VR Unity Developer&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;notes-from-initial-discussion&quot;&gt;Notes from initial discussion&lt;/h2&gt;

&lt;h1 id=&quot;brief-outline---project-background&quot;&gt;Brief outline - project background&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;StoryFutures Academy&lt;/strong&gt; - one of the government’s appointed ‘creative clusters’. Collaboration between Royal Holloway University of London and National Film and Television School, who are primaritly interested in funding and upskilling businesses and organisations in the West London area in new digital media - VR, AR, XR etc.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Train The Trainer&lt;/strong&gt; is a StoryFutures initiative as part of a funding strand offered to work with universities across the country.&lt;/li&gt;
  &lt;li&gt;Five universities have this funding - Falmouth, Hertfordshire, Abertay, UCL, Bath Spa.&lt;/li&gt;
  &lt;li&gt;The funding is, in this case, to be used for a &lt;strong&gt;‘teaching-led’&lt;/strong&gt; project.&lt;/li&gt;
  &lt;li&gt;Due to the COVID-19 pandemic, this year Falmouth University &lt;strong&gt;AMATA&lt;/strong&gt; (Academy of Music and Theatre Arts) are devising a project that is about creating for an immersive, virtual environment.&lt;/li&gt;
  &lt;li&gt;The view of the directors is that theatre, performance, acting students - those interested in live performance - are better placed to consider such spaces than film and television students, because they are used to working in spaces that happen all around, be they real or virtual.&lt;/li&gt;
  &lt;li&gt;We will be working with &lt;strong&gt;theatre and performance&lt;/strong&gt; students and &lt;strong&gt;technical theatre arts&lt;/strong&gt; students.&lt;/li&gt;
  &lt;li&gt;The students will be engaged in creating content for the environment.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;20&lt;/strong&gt; students in total: &lt;strong&gt;16&lt;/strong&gt; theatre and performance and &lt;strong&gt;4&lt;/strong&gt; technical theatre arts.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;the-environment&quot;&gt;The environment&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Important: the &lt;strong&gt;‘liveness’&lt;/strong&gt; and &lt;strong&gt;‘interactiveness’&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Visual concept - the &lt;strong&gt;‘varyon’&lt;/strong&gt; - morphs and moves around the audience.&lt;/li&gt;
  &lt;li&gt;As well as images of performers streamed into the environment, they would like some form of &lt;strong&gt;‘volumetric capture’&lt;/strong&gt; if possible, where the performers appear in the space as a kind of &lt;strong&gt;‘hologram’&lt;/strong&gt;, or &lt;strong&gt;‘avatar’&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;This can be incomplete, blocky or outlined etc. - i.e.  not a full perfect representation.&lt;/li&gt;
  &lt;li&gt;Audience will wear headsets&lt;/li&gt;
  &lt;li&gt;Performers streamed in via, for example, cameras or laptops.&lt;/li&gt;
  &lt;li&gt;Interested in how an audience with audience interaction might &lt;em&gt;look like&lt;/em&gt;, how would they be aware of each other’s presence, how will they interact with the space?&lt;/li&gt;
  &lt;li&gt;Audience of approximately 8-10.&lt;/li&gt;
  &lt;li&gt;One idea is to maybe pick four formations created by the screens, and the audience transitions between them via an animation, with each space being seen as an interactive ‘installation’ space.&lt;/li&gt;
  &lt;li&gt;Klaus Kruse (director) outlined possible ideas for ‘flow’ through the space(s).&lt;/li&gt;
  &lt;li&gt;Potential for audience members to split up and explore difference spaces before joining together in a joint &lt;em&gt;‘grand finale’&lt;/em&gt; / &lt;em&gt;‘communal conclusion’&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;There is an awareness of the possible limitations and realisation that not every idea may be possible, or feasible given the time and technical limitations.&lt;/li&gt;
  &lt;li&gt;The animations can be derived from the experience of the way the screens, each of which hinges onto the next, move when the space changes its shape.&lt;/li&gt;
  &lt;li&gt;It would be hoped that the audience can interact with the screens in at least one of the spaces - push, move, realign into different shapes.&lt;/li&gt;
  &lt;li&gt;Live interaction via video so the audience can see and speak with the performers - live feed.&lt;/li&gt;
  &lt;li&gt;This would also mean the space can be used as a live rehearsal space.&lt;/li&gt;
  &lt;li&gt;The audience sees the performer on the screen.&lt;/li&gt;
  &lt;li&gt;Pixelation etc. will be ‘embraced’ and become part of the aesthetic.&lt;/li&gt;
  &lt;li&gt;Students can maybe access the space via 360 degree video displayed on google cardboard via phones.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/GAM750/not-near-enough-perf-map-1.jpg&quot; alt=&quot;performance map&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;research-experimentation-dialogue-and-rd&quot;&gt;Research, experimentation, dialogue and R&amp;amp;D&lt;/h1&gt;

&lt;p&gt;This is very much a dialogue, a collaboration. There is space for me, as a developer to bring to the project. It is likely that, as theatre practitioners, the project devisers and leaders of the project have thought aobut differentely, or have ideas that are not necessarily feasible in the way they imagined them. This project is in many respects, an R&amp;amp;D project - there is a lot of research and experimentation to be done in attempting to realise something that is close to the original concepts.&lt;/p&gt;

&lt;p&gt;There is also the potential for publishing a research paper or two at the end of the project. It this stage, I see this as something I’d be interested in being invovled in.&lt;/p&gt;

&lt;h1 id=&quot;time-scale&quot;&gt;Time scale&lt;/h1&gt;

&lt;p&gt;They would like something ready and working - certainly not complete - during September, or start of October, in order to work with students within the virtual space.&lt;/p&gt;

&lt;h1 id=&quot;collaboration&quot;&gt;Collaboration&lt;/h1&gt;

&lt;p&gt;It was agreed that, should it be necessary or helpful to bring in external expertise as and when necessary, we will do so. I’m thinking potentially in terms of graphics, animations, character or environment aesthetics etc., that this could be a good thing to do.&lt;/p&gt;</content><author><name></name></author><summary type="html">Not Near Enough is a StoryFutures Academy-funded project at Falmouth University exploring VR theatre spaces.</summary></entry><entry><title type="html">Normcore</title><link href="http://localhost:4000/2020/07/13/normcore-1.html" rel="alternate" type="text/html" title="Normcore" /><published>2020-07-13T00:00:00+01:00</published><updated>2020-07-13T00:00:00+01:00</updated><id>http://localhost:4000/2020/07/13/normcore-1</id><content type="html" xml:base="http://localhost:4000/2020/07/13/normcore-1.html">&lt;p&gt;This is potentially the best option of all - if I were to forgoe the build-your-own-networking-solution learning curve I mention in the &lt;strong&gt;&lt;a href=&quot;/2020/07/11/client-server-tutorial.html&quot;&gt;previous post&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://normcore.io/&quot;&gt;Normcore&lt;/a&gt;&lt;/strong&gt; looks to be self-contained, including all the building block APIs to get a fully working multiplayer up and running in a short space of time, much like Photon but, from the looks of it, even simpler to implement. It’s main attraction is that it is particularly geared toward VR/AR, which is a huge part of the work out of the way.&lt;/p&gt;

&lt;p&gt;However, it is less mature than Photon, being currently in Beta. But that makes it nice, fresh and young!&lt;/p&gt;

&lt;p&gt;It is created by &lt;strong&gt;&lt;a href=&quot;https://www.normalvr.com/&quot;&gt;normal&lt;/a&gt;&lt;/strong&gt;, who create VR games themselves. I have been particularly enthral to &lt;strong&gt;&lt;a href=&quot;https://halfandhalf.fun/&quot;&gt;Half+Half&lt;/a&gt;&lt;/strong&gt;. Normcore is the plugin they have build for themselves to build their software. It’s all explained in their &lt;strong&gt;&lt;a href=&quot;https://www.normalvr.com/blog/normcore/&quot;&gt;blog&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We’ve spent the last three years working on Normcore, a Unity plug-in for our own internal use, implementing all the different pieces—state syncing, physics syncing, voice chat, persistence, fast serialization with versioning, delta compression, flow control, and much more. Through this process, we noticed a pattern: Everyone currently needs to implement each of these pieces from scratch.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;…we’ve noticed the VR/AR community is in serious need of good multiplayer support, especially when it comes to voice chat (which we believe is paramount to achieving presence in multiplayer spaces). The VOIP community solved high-quality low-latency voice chat a long time ago, and we’ve incorporated the lessons they’ve learned into how audio works in Normcore&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So… all very, very interesting…&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\normcore-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;</content><author><name></name></author><summary type="html">This is potentially the best option of all - if I were to forgoe the build-your-own-networking-solution learning curve I mention in the previous post.</summary></entry></feed>