<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-10-25T15:29:51+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Creative Reflective Journal</title><subtitle>My Creative Reflective Journal towards the Falmouth University Creative App Development Masters Degree.</subtitle><entry><title type="html">Meet Me On The Other Side</title><link href="http://localhost:4000/2020/10/25/meet-me-on-the-other-side.html" rel="alternate" type="text/html" title="Meet Me On The Other Side" /><published>2020-10-25T00:00:00+01:00</published><updated>2020-10-25T00:00:00+01:00</updated><id>http://localhost:4000/2020/10/25/meet-me-on-the-other-side</id><content type="html" xml:base="http://localhost:4000/2020/10/25/meet-me-on-the-other-side.html">&lt;h1 id=&quot;pervasive-media-studio&quot;&gt;Pervasive Media Studio&lt;/h1&gt;

&lt;p&gt;The Pervasive Media Studio is a partnership between Watershed, UWE Bristol and University of Bristol.&lt;/p&gt;

&lt;p&gt;https://www.youtube.com/watch?v=yt0Hnxz31V0&lt;/p&gt;

&lt;p&gt;Interesting talk about VR, immersion, VR experience&lt;/p&gt;

&lt;p&gt;Opposiing concepts: Cartesian Metaphysics (Descart) / Embodied Cognition&lt;/p&gt;

&lt;p&gt;Social platofmr tent to attract social groupins that match other social contexts&lt;/p&gt;

&lt;p&gt;Altspace (19:30)&lt;/p&gt;

&lt;p&gt;Hubs by Mozilla (22.30)
    no headset required
    more about creating your own personona
    more control, build from scratch
    challenging&lt;/p&gt;</content><author><name></name></author><summary type="html">Pervasive Media Studio</summary></entry><entry><title type="html">Multispace 2</title><link href="http://localhost:4000/2020/10/24/multi-space-2.html" rel="alternate" type="text/html" title="Multispace 2" /><published>2020-10-24T00:00:00+01:00</published><updated>2020-10-24T00:00:00+01:00</updated><id>http://localhost:4000/2020/10/24/multi-space-2</id><content type="html" xml:base="http://localhost:4000/2020/10/24/multi-space-2.html">&lt;h1 id=&quot;creating-multiple-scenes-from-an-orginal-template&quot;&gt;Creating multiple scenes from an orginal template&lt;/h1&gt;

&lt;p&gt;Having given the concept of mutlple spaces some thought, where each space represents a &lt;em&gt;scene&lt;/em&gt; in the multiplayer theatre environment, it became clear that I could make use of the existing infrastructure I have already put in place (see &lt;strong&gt;&lt;a href=&quot;/2020/09/01/creating-the-screens-1.html&quot;&gt;Creating the screens 1&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&quot;/2020/09/03/creating-the-screens-2.html&quot;&gt;Creating the screens 2&lt;/a&gt;&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;This method uses a class model for each screen formation, comprising definitions for the postition and rotation of each of the sixteen screen display panels within the respective formation. This information is then used when instantiating, or &lt;em&gt;spawning&lt;/em&gt;, the formation. Animations are created by &lt;em&gt;tweening&lt;/em&gt; between the previous formation’s positioning data and that of the next formation. This has, thus far, been happening in a single location, i.e. that set in each formation’s respective model.&lt;/p&gt;

&lt;p&gt;Taking this concept and applying it to the creating of a second (or third or fourth etc.) formation, it becomes clear that the existing definitions can be used as a template, with an &lt;em&gt;offset&lt;/em&gt; applied in order to translate the positional information from the original formation to another in another place within the world space.&lt;/p&gt;

&lt;p&gt;This approach requires a set of definitions for the locations of each scene within the world space. If the original scene is set at &lt;em&gt;Vector3(0, 0, 0)&lt;/em&gt;, then the second scene can exist at &lt;em&gt;Vector3(20, 0, 0)&lt;/em&gt;, and the third at &lt;em&gt;Vector3(20, 0, 20)&lt;/em&gt; etc. 
It is then simply a case of applying this information in order to offset the original formation’s positional data. The rotations must remain the same as we do not want the screens to change direction!
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scene position definitions within the world space&lt;/strong&gt;&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;\images\GAM750\GetScenePosition-1.JPG&quot; alt=&quot;GetScenePosition&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Screen formations, showing screen positions offset by selected scene postition&lt;/strong&gt;&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;\images\GAM750\ScreenFormationService-1.JPG&quot; alt=&quot;ScreenFormationService&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scene detail model, showing how each scene comprises a scene position in world space, a screen formation definition, and references to the actual screens that have been instantiated&lt;/strong&gt;&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;\images\GAM750\SceneDetailModel.JPG&quot; alt=&quot;SceneDetailModel&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Screen position model for each individual screen&lt;/strong&gt;&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;\images\GAM750\ScreenFormationModel.JPG&quot; alt=&quot;SceneDetailModel&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Transversing the multispace&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\GAM750\transverse-multispace-2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Code behind the multispace&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\GAM750\multispace-code-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;</content><author><name></name></author><summary type="html">Creating multiple scenes from an orginal template</summary></entry><entry><title type="html">Multispace 1</title><link href="http://localhost:4000/2020/10/23/multi-space-1.html" rel="alternate" type="text/html" title="Multispace 1" /><published>2020-10-23T00:00:00+01:00</published><updated>2020-10-23T00:00:00+01:00</updated><id>http://localhost:4000/2020/10/23/multi-space-1</id><content type="html" xml:base="http://localhost:4000/2020/10/23/multi-space-1.html">&lt;p&gt;&lt;img src=&quot;\images\GAM750\lightbulb.jpg&quot; alt=&quot;Lightbulb&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;thinking-about-multiple-spaces-or-multiple-universes&quot;&gt;Thinking about multiple spaces (or multiple universes)&lt;/h1&gt;

&lt;p&gt;I’ve just had an interesting thought, as a respone to the problem of creating multiple &lt;em&gt;‘spaces’&lt;/em&gt;, which players can move between, at times that will be decided later.&lt;/p&gt;

&lt;p&gt;If an &lt;em&gt;‘environment’&lt;/em&gt; can be spawned as an object, and accessed through a set of methods and properties etc., then this will greatly ease the way in which a player can be ‘dropped’ into the space and interect with the objects that are present within it.&lt;/p&gt;

&lt;p&gt;In a way, this is similar to creating multiple &lt;em&gt;scenes&lt;/em&gt; in Unity, except that these exist as entities within the same single scene.  In this form, therefore, they can exploit an outward facing &lt;em&gt;interface&lt;/em&gt; in order to interact with a Master Game Controller-type object, that controls the manner of who, what, why, and where a player can exist and function, but only within the specific space that it exists.&lt;/p&gt;

&lt;h1 id=&quot;nice-theory-what-about-the-actualities&quot;&gt;Nice theory, what about the actualities?&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Negative: Could ge difficult at this stage of development, having already put much infrastructure into place.&lt;/li&gt;
  &lt;li&gt;Positive: It is actually fairly early in development terms given a longer view on the project as a whole and its potential as we move forward&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It would be a task that would include creating an interface for the space, in order to access multiple other interfaces, i.e. per method an property that is part of that space.&lt;/p&gt;

&lt;h1 id=&quot;but-no&quot;&gt;But… no!&lt;/h1&gt;

&lt;p&gt;This idea would not work - I think - because the VR experience is seen from a &lt;strong&gt;first person&lt;/strong&gt; perspective, and all code is run in a similar manner. That is, each individual instance of Unity, running in each headset, will access its own set of game objects and, on each object, a set of components. That individual is &lt;strong&gt;not&lt;/strong&gt; controlling from heaven, but from their own person… on the ground.  The &lt;em&gt;‘God’&lt;/em&gt; concept suggested above, i.e. where control is exerted down on the multispace and controlling eacn individual multiplace, could work as part of a controller interface for the use of &lt;strong&gt;Stage Managers&lt;/strong&gt; and similar roles. But this is looking too complex for this purpose and within the available time frame.&lt;/p&gt;

&lt;p&gt;Therefore, I will maintain this post as a reminder of a possible way to think of this Potential model in the future.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Spatial Audio With Normcore</title><link href="http://localhost:4000/2020/10/18/spatial-audio-with-normcore.html" rel="alternate" type="text/html" title="Spatial Audio With Normcore" /><published>2020-10-18T01:00:00+01:00</published><updated>2020-10-18T01:00:00+01:00</updated><id>http://localhost:4000/2020/10/18/spatial-audio-with-normcore</id><content type="html" xml:base="http://localhost:4000/2020/10/18/spatial-audio-with-normcore.html">&lt;h1 id=&quot;out-of-the-box-audio-appears-to-be-2d&quot;&gt;Out-of-the-box: audio appears to be 2D!&lt;/h1&gt;
&lt;p&gt;During the first realtime testing with the project’s expert stakeholders, i.e. the director and technical director, I was eager to see if audio between players is 3D / spatialised, i.e. can be used to sense distance and direction. Unfortunately, it appeared to be very much 2D and unspatialised; there was no volume roll-off - no matter how near or far apart we were from each other, the volume of the voice was constant.&lt;/p&gt;

&lt;p&gt;This is a real shame.&lt;/p&gt;

&lt;p&gt;I am now looking into possible solutions for this. I am hoping that is will be as straightforward as a simple setting somewhere, or a case of adding a 3D sound source to the VR player.&lt;/p&gt;

&lt;h1 id=&quot;testing-two-headsets-are-better-than-one&quot;&gt;Testing: two headsets are better than one&lt;/h1&gt;
&lt;p&gt;Question: How do you test this with only a single headset? 
Answer: With great difficulty!&lt;/p&gt;

&lt;p&gt;Testing multiplayer environments is not the only reason I bought a new Oculus Quest 2, but it is the main one. Or, at least, it’s a good excuse!&lt;/p&gt;

&lt;p&gt;Not only can I now experience multiplayer applicaitons with family and friends, but I can also test my own developments. Not to mention that the latest model is much faster and has a much better display, at a cheaper price point. Seemed to make sense to me!&lt;/p&gt;

&lt;p&gt;Setting up is a bit of a problem, though, because Facebook now insist that you use a FB account! It is not even possible just to use your old Oculus account - it now must be linked to a FB account, even if you continue to use it only on your current headset. Bad, bad Facebook! This means that, in order to play multiplayer, &lt;strong&gt;I need to sign in on different accounts&lt;/strong&gt;. For this reason, I set up a second FB account attached to a second user on my Android phone. I will have to log into each, separately, and use each account to log into each headset, separately. What a faff!&lt;/p&gt;

&lt;p&gt;Back to the spacial audio issue…&lt;/p&gt;

&lt;h1 id=&quot;first-port-of-call-normcore-discord-server&quot;&gt;First port of call: Normcore Discord server&lt;/h1&gt;

&lt;p&gt;[01-05-2019]&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;if you’re using the built-in spatializer, then you can fix easily by changing this line in AudioOutput.cs:&lt;/p&gt;

  &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;data[sOut] = !_mute ? audioData[sIn] : 0.0f;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;just change that to this:&lt;/p&gt;

  &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;data[sOut] *= !_mute ? audioData[sIn] : 0.0f;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;the input value to that OnAudioFilterRead() is set up to be 1.0 * whatever attenuation Unity has added to the AudioSource including volume&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;[01-09-2020]&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;I think in V1 I had disabled the built-in spatialization&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;[02-09-2020]&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Question: How do I make the player voice 3D? (e.g. how do i make them louder the closer you are to them, and quieter when you are further away? - by default you can hear people constantly)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;[02-09-2020]&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;take a look at the audio source on your avatar (make sure the “spatial” slider is set to “3d”) and you can muck with the falloff values on there if you want to customize it.
Unity defaults to 2D, and the falloff range is basically “100% volume up to 5 meters, still audible 100 meters away” or something like that, so it’s definitely tuned for larger spaces.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;[02-09-2020]&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;if you add an audio source RealtimeAvatarVoice will detect it and use it instead of creating one
you can use SetActive, but you’ll also need to make a custom RealtimeComponent in order to synchronize the active state. Normcore doesn’t synchronize that part for you&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;[04-09-2020]&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;I followed your advice to add an audio source to the RealtimeAvatarVoice gameobject in order to allow the spatial blend of the player’s voice to be 3d, however its still behaving like its 2D - any idea on what could be wrong?
general/04-09-2020&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;[04-09-2020]&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;if you’re on V1, try making this change:
&lt;a href=&quot;if you’re on V1, try making this change:
https://discord.com/channels/393839515074297858/393841777091543052/573241867185946680&quot;&gt;if you’re on V1, try making this change:
https://discord.com/channels/393839515074297858/393841777091543052/573241867185946680
&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name></name></author><summary type="html">Out-of-the-box: audio appears to be 2D! During the first realtime testing with the project’s expert stakeholders, i.e. the director and technical director, I was eager to see if audio between players is 3D / spatialised, i.e. can be used to sense distance and direction. Unfortunately, it appeared to be very much 2D and unspatialised; there was no volume roll-off - no matter how near or far apart we were from each other, the volume of the voice was constant.</summary></entry><entry><title type="html">Building The Apps</title><link href="http://localhost:4000/2020/10/01/building-the-apps.html" rel="alternate" type="text/html" title="Building The Apps" /><published>2020-10-01T03:00:00+01:00</published><updated>2020-10-01T03:00:00+01:00</updated><id>http://localhost:4000/2020/10/01/building-the-apps</id><content type="html" xml:base="http://localhost:4000/2020/10/01/building-the-apps.html">&lt;h1 id=&quot;audience---oculus-quest&quot;&gt;Audience - Oculus Quest&lt;/h1&gt;

&lt;h1 id=&quot;performer-main-room---pcmac&quot;&gt;Performer Main Room - PC/Mac&lt;/h1&gt;

&lt;p&gt;PC build works. 
Mac build creates exceptions - I believe these can be traced to requirements for the &lt;strong&gt;info.plist&lt;/strong&gt; - i.e. requesting permissions&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Microphone class is used but Microphone Usage Description is empty. App will not work on macOS 10.14+.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Search leads to responses such as &lt;a href=&quot;https://answers.unity.com/questions/1735434/build-error-microphone-class-is-used-but-microphon.html&quot;&gt;You need to add a microphone usage description string in iOS Player Settings (which will be added to your Info.plist)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://developer.apple.com/documentation/avfoundation/cameras_and_media_capture/requesting_authorization_for_media_capture_on_macos?language=objc&quot;&gt;Requesting Authorization for Media Capture on macOS&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I need the mike, so this is a requirement.&lt;/p&gt;

&lt;p&gt;I found all files named info.plist and added the line, where it was missing:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;key&gt;NSMicrophoneUsageDescription&lt;/key&gt;
  &lt;string&gt;Yes&lt;/string&gt;
&lt;/blockquote&gt;

&lt;p&gt;However, this did not work - I still get the microphone error.&lt;/p&gt;

&lt;h1 id=&quot;performer-video-streaming---pcmac&quot;&gt;Performer Video Streaming - PC/Mac&lt;/h1&gt;

&lt;h1 id=&quot;stage-manager-osc-controller---pcmac&quot;&gt;Stage Manager OSC Controller - PC/Mac&lt;/h1&gt;

&lt;p&gt;The PC apps need to be built for Mac - it seems the world of Theatre runs on Macs!&lt;/p&gt;</content><author><name></name></author><summary type="html">Audience - Oculus Quest</summary></entry><entry><title type="html">Screen Panel Trigger Button</title><link href="http://localhost:4000/2020/10/01/screen-panel-trigger-button.html" rel="alternate" type="text/html" title="Screen Panel Trigger Button" /><published>2020-10-01T02:00:00+01:00</published><updated>2020-10-01T02:00:00+01:00</updated><id>http://localhost:4000/2020/10/01/screen-panel-trigger-button</id><content type="html" xml:base="http://localhost:4000/2020/10/01/screen-panel-trigger-button.html">&lt;h1 id=&quot;the-screen-is-now-a-button&quot;&gt;The screen is now a ‘button’&lt;/h1&gt;

&lt;p&gt;When the player’s hand touches the screen, one of two actions happen, randomly: -&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A randomly selected video clip plays&lt;/li&gt;
  &lt;li&gt;An animation occurs into a new randomly selected screen formation&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This can be fairly startling, particularly the sudden change to screen formation, even more so when uninitiated and unexpected.&lt;/p&gt;

&lt;p&gt;The number and type of different actions can be added to as development continues.&lt;/p&gt;

&lt;p&gt;One idea would be to have some control of which of the actions happens, for example, by specific screen or by specific players. The latter would require identification of individual players, or of &lt;em&gt;‘type’&lt;/em&gt; of player, in the first place - again, something that can be added in a future development.&lt;/p&gt;</content><author><name></name></author><summary type="html">The screen is now a ‘button’</summary></entry><entry><title type="html">The Performer App 2</title><link href="http://localhost:4000/2020/10/01/the-performer-app-2.html" rel="alternate" type="text/html" title="The Performer App 2" /><published>2020-10-01T01:00:00+01:00</published><updated>2020-10-01T01:00:00+01:00</updated><id>http://localhost:4000/2020/10/01/the-performer-app-2</id><content type="html" xml:base="http://localhost:4000/2020/10/01/the-performer-app-2.html">&lt;h1 id=&quot;problem-solved&quot;&gt;Problem solved&lt;/h1&gt;

&lt;p&gt;I created a copy of the main VR scene and reset the UUID of the Normcore Realitime View (Script) attached to the MediaDisplayManager GameObject.&lt;/p&gt;

&lt;p&gt;The MediaDisplayManager exists to house the management classes that control media-related objects and actions, such as the screen panels, e.g. video screen selection and media assignment, the Agora video streaming controller and interface, and other related management tasks - simply, a kind of Game Manager but specifically for controlling media in the widest sense.&lt;/p&gt;

&lt;p&gt;I then replaced the VR player with another, exactly the same but using an ‘invisible’ copy of the OVR player, i.e. I disabled the body and just kept the camera and Realtime components to persist the player in the multiplayer environment. I added a Rigidbody and a script to allow control via the keyboard rather than VR.&lt;/p&gt;

&lt;p&gt;When this scene is built, it must have the VR Plugin Management disabled in Player Settings. This then creates an exact copy of the VR scene, in the multiplayer environment, but the player is invisible and unable to affect any change or force on the space - a ghost. When a PC build is created it runs on another PC - the VR character(s) in the space can be viewed along with the results of their actions, i.e. screen/video/formation selection and assignment changes etc., whilst the invisible player can move around using the keyboard.&lt;/p&gt;</content><author><name></name></author><summary type="html">Problem solved</summary></entry><entry><title type="html">The Performer App 1</title><link href="http://localhost:4000/2020/09/27/the-performer-app-1.html" rel="alternate" type="text/html" title="The Performer App 1" /><published>2020-09-27T01:00:00+01:00</published><updated>2020-09-27T01:00:00+01:00</updated><id>http://localhost:4000/2020/09/27/the-performer-app-1</id><content type="html" xml:base="http://localhost:4000/2020/09/27/the-performer-app-1.html">&lt;h1 id=&quot;how-should-the-performer-experience-the-vr-space&quot;&gt;How should the performer experience the VR space?&lt;/h1&gt;
&lt;p&gt;I had been wondering for some time how I would approach the issue of enabling the performers, who are streaming live video into the VR space to be projected onto the panel screens, to &lt;strong&gt;also be able to see what is happening within the space&lt;/strong&gt;, where audience members are located, where they are looking, how they are moving; the screen formations and how they morph into different shapes; lighting; other performers and where they are beging displayed etc.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rendering video via WebRTC&lt;/strong&gt;&lt;br /&gt;
I had assumed that I would need to stream video, or &lt;em&gt;volumetric capture&lt;/em&gt;, from the environment to be displayed, for example, on a browser or some other display. I imagined this would make use of WebRTC in a similar way as streaming live video of a performer into the VR space. I’m sure it would be possible, but it would be time consuming - not something I would relish after having already spent a significant amount of effort and time on the stack put in place thus far. However, it would certainly be an option for further development if required - Oculus has developed a &lt;strong&gt;&lt;a href=&quot;https://forum.unity.com/threads/unity-render-streaming-introduction-faq.742481/&quot;&gt;streaming solution&lt;/a&gt;&lt;/strong&gt; (the GitHub repository is &lt;strong&gt;&lt;a href=&quot;https://github.com/Unity-Technologies/UnityRenderStreaming&quot;&gt;here&lt;/a&gt;)&lt;/strong&gt;, so it’s not unprecedented.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sending positional data to the performer/client&lt;/strong&gt;&lt;br /&gt;
Then I began to think of streaming pure location data for performers and screens etc to a separate app, where they could be used to map the space in real time.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Joining the performer into the multiplayer space&lt;/strong&gt;&lt;br /&gt;
However, the most immediately straightforward option would also seem to be potentially the most useful: to join the performer into the multiplayer space as an invisible player, who can roam around with absolute freedom, but who’s presence cannot be perceived by the audience. The performer would not be experiencing the space through VR, but on a 2D flat screen, and controlling their movements using keyboard or mouse. I did briefly experiment with just having an overhead camera above the space, but this was soon replaced with the more immersive option of being in the space itself.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Overhead Camera&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\GAM750\overhead-camera-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Performer ‘in space’&lt;/strong&gt;&lt;br /&gt;
Here the performer is in the space with a single VR player (i.e. audience). This is what it could look like when it’s working - however, I am currently having some issues with Normcore allowing this to happen - it sometimes crashes out, loses connection, or freezes for the performer app. Sometimes it just does not follow actions from the VR environment. It may be that there is not enough separation, somehow, between the player, their Normcore ‘realtime’ player, their Oculus avatar.. or at least somewhere in there there is an issue - tracking it is proving difficult. But the video below at least shows what could be possible!&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\GAM750\performer-space-2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;And here’s an example of an ‘error’&lt;/strong&gt;&lt;br /&gt;
  ..the actions on the left (i.e. VR), are not propergated on the right (non-VR)&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\GAM750\performer-space-err-2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;</content><author><name></name></author><summary type="html">How should the performer experience the VR space? I had been wondering for some time how I would approach the issue of enabling the performers, who are streaming live video into the VR space to be projected onto the panel screens, to also be able to see what is happening within the space, where audience members are located, where they are looking, how they are moving; the screen formations and how they morph into different shapes; lighting; other performers and where they are beging displayed etc.</summary></entry><entry><title type="html">Sending and Receiveing OSC Data</title><link href="http://localhost:4000/2020/09/14/sending-data.html" rel="alternate" type="text/html" title="Sending and Receiveing OSC Data" /><published>2020-09-14T01:00:00+01:00</published><updated>2020-09-14T01:00:00+01:00</updated><id>http://localhost:4000/2020/09/14/sending-data</id><content type="html" xml:base="http://localhost:4000/2020/09/14/sending-data.html">&lt;p&gt;I’ve been thinking about how to send data between clients. The requirement is to make use of one of the two platforms I have put in place to keep clients connected - i.e. &lt;strong&gt;Normcore&lt;/strong&gt; for multiplayer functionality and &lt;strong&gt;Agora&lt;/strong&gt; for live video and audio streaming - in order to send simple data that can be used and interpreted in a number of ways, with a number of possible functionalities in mind.&lt;/p&gt;

&lt;h1 id=&quot;osc-addressing&quot;&gt;OSC addressing&lt;/h1&gt;
&lt;p&gt;The main one here is &lt;strong&gt;OSC&lt;/strong&gt; messages, used to control screen content, formations, lighting, sound, and many other potential uses that could come up as this project continues to be developed. As this data is simply text, it could be passed in form of JSON or raw strings, and then easily interpreted by clients connected in.&lt;/p&gt;

&lt;p&gt;But there could potentially be other uses for such data channels. Perhaps they could be used to send &lt;em&gt;coordinate&lt;/em&gt; data for the screen formations and, more interestingly, the &lt;em&gt;players within the VR space&lt;/em&gt;. These could then be used in mappying out the space in order for the performers to keep tabs on where players are and on their movements.  However, I’m not sure if this will be close enough to &lt;em&gt;realtime&lt;/em&gt; to work sufficiently well.&lt;/p&gt;

&lt;h1 id=&quot;agora&quot;&gt;Agora&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Agora&lt;/strong&gt; provides a simple data channel that can be used for sending text messages from broadcaster to clients. I am currently thinking that this could be very useful for sending data in JSON format.&lt;/p&gt;

&lt;p&gt;I have tested it with simple &lt;em&gt;‘hello’&lt;/em&gt; messages.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Message sent from Video Streaming client and received by VR client&lt;/strong&gt;
&lt;img src=&quot;\images\GAM750\agora-data-stream-message-1.JPG&quot; alt=&quot;message 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Message sent from VR client and received by Video Streaming client&lt;/strong&gt;
&lt;img src=&quot;\images\GAM750\agora-data-stream-message-2.JPG&quot; alt=&quot;message 2&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;normcore&quot;&gt;Normcore&lt;/h1&gt;
&lt;p&gt;I am very tempted to hook up both the video streaming and the OSC controller client apps to Normcore and allow data to be pushed around using Normcore data-pooling techniques. This could work in exactly the same as does sending video, screen and screen formation selection currently does between clients that have joined the room. The advantage of this method could be that it would be a relatively simple task to share &lt;strong&gt;all&lt;/strong&gt; data that is being used by &lt;strong&gt;all&lt;/strong&gt; of the client apps involved. It would not require specific extra channels, methodologies and means of sending and receiving data. The way the data is packaged and then interpreted at the receiving end will be the same, but pooling the data via Normcore will allow it to be accessible by any client at any time.&lt;/p&gt;

&lt;h1 id=&quot;decision&quot;&gt;Decision?&lt;/h1&gt;
&lt;p&gt;The simplicilty of sending text messages using Agora is tempting me in this direction. There is little to set up and it will not require writing methods to keep the data in sync between clients.&lt;/p&gt;

&lt;p&gt;I feel the way forward will be to include both Normcore and Agora in all three apps, so that any can be called upon where necessary. But for the time being, I will explore the use of Agora’s simple messaging ability for sending OSC messages.&lt;/p&gt;

&lt;p&gt;When it comes to finding ways to keep performers (i.e. video streaming clients) informed as to the positioning of audience, it may be that I will turn to Normcore.&lt;/p&gt;</content><author><name></name></author><summary type="html">I’ve been thinking about how to send data between clients. The requirement is to make use of one of the two platforms I have put in place to keep clients connected - i.e. Normcore for multiplayer functionality and Agora for live video and audio streaming - in order to send simple data that can be used and interpreted in a number of ways, with a number of possible functionalities in mind.</summary></entry><entry><title type="html">The Possibilities Of OSC</title><link href="http://localhost:4000/2020/09/13/osc-possibilities.html" rel="alternate" type="text/html" title="The Possibilities Of OSC" /><published>2020-09-13T01:00:00+01:00</published><updated>2020-09-13T01:00:00+01:00</updated><id>http://localhost:4000/2020/09/13/osc-possibilities</id><content type="html" xml:base="http://localhost:4000/2020/09/13/osc-possibilities.html">&lt;h1 id=&quot;osc-controller&quot;&gt;OSC Controller&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;\images\GAM750\osc-interface-2.JPG&quot; alt=&quot;Formation 5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I created an Unity project for the purpose of developing an &lt;em&gt;OSC controller&lt;/em&gt;. After importing the &lt;strong&gt;OSC Simpl&lt;/strong&gt; Unity package, I studied the example apps and the documentation - which is concise but not rich enough to rely on without a fair bit of code-analysis - and managed to cobble together an OCS controller with buttons that replace the temporary VR interface I have been using to control the environment from within.&lt;/p&gt;

&lt;p&gt;The main app also contains the OSC package and listens for OSC data messages. Once received, the message fires exactly the same actions as the temporary interface. Therefore, the desired effects not only occur, but they sync across &lt;em&gt;Normcore&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Currently, the OSC messages are in the form: -&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;/select/video&lt;/li&gt;
  &lt;li&gt;/select/stream&lt;/li&gt;
  &lt;li&gt;/select/display&lt;/li&gt;
  &lt;li&gt;/select/formation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The /select/ part indicates that a selection has been made.
/video/, /stream/ etc. indicates what kind of action we are going to request.&lt;/p&gt;

&lt;p&gt;Following this, an argument is passed indicating the &lt;strong&gt;ID&lt;/strong&gt; of the video, the video stream, the display screen, or the screen formation…&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    const string _videoSelectAddress = &quot;/select/video&quot;;
    const string _streamSelectAddress = &quot;/select/stream&quot;;
    const string _displaySelectAddress = &quot;/select/display&quot;;
    const string _formationSelectAddress = &quot;/select/formation&quot;;

    public void VideoSelect(int videoId)
    {
        oscOut.Send(_videoSelectAddress, videoId);
	    }
    public void StreamSelect(int streamId)
    {
        oscOut.Send(_streamSelectAddress, streamId);
    }
	public void DisplaySelect(int displayId)
    {
        oscOut.Send(_displaySelectAddress, displayId);
	    }
    public void FormationSelect(int formationId)
    {
        oscOut.Send(_formationSelectAddress, formationId);
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;… and at the listening end, these patterns are looked for and responces are wired up. Therefore a specific message plus arguments is sent, and this results in a specific method being called at the receiver’s end.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\images\GAM750\osc-interface-1.JPG&quot; alt=&quot;OSC Interface&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\GAM750\osc-unity-demo-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\GAM750\osc-demo-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;should-be-fun&quot;&gt;Should be fun&lt;/h1&gt;

&lt;p&gt;Of course, this follows the pattern already in place in the main VR app, i.e. when selecting the video or stream to display, the ID is selected first, followed by the ID of the display on which the respective media is to be shown.&lt;/p&gt;

&lt;p&gt;However, as suggested by my fellow student and resident OSC expert, Sam Smallman, on our Discord server forum, there are other possible ways to provide this information: -&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Sam:&lt;/strong&gt; You can use wildcards to help so you would basically receive an OSC message and pattern match for “/fireclown/screen/*/hidden” then you would grab the 3rd part which would be the screen number and then take the argument value and hide unhide the screen from there.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Have you considered stateless commands, so /formation 3 “1 2 3 4 5”, basically set formation to 3(1st argument) to the screens 1 thru 5 (2nd string argument)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;then you could have /screen/1/formation 3, to set indivual screen 1 to formation 3
.. ah no because formation is a global thing but video feed, /screen/1/video 3, to set indivdual screen 1 to video feed 3. There’s a lot of fun you can have with this&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Me:&lt;/strong&gt; My big issue, though, is how to set it up to send over the internet. Probably have to send it via Agora - which I’m using for the video streaming but also has a data channel - or normcore somehow… I think you’ve mentioned before that theatre is usually done in a closed network.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Sam:&lt;/strong&gt; it might be easier to just send http post requests with a json packet as the arguments, and then you can translate them to osc&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;oh yeah, most theatre wont be online ever. Things are changing at the moment but still there’s massive superstition about putting anything online during a performance.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Me:&lt;/strong&gt; As this project is all about remote working, I have no choice in the matter.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Sam:&lt;/strong&gt; you would need to wrap osc anyway to get over the net you may aswell just wrap it in http&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;as a theatre practitioner my next questions would be what control over the screens do i have. Does screen feeds snap from one to another, do i have control of fading it,  do i have control of the formations and how they move from one to another and the time they take, can i indivdually control screen brightness etc. All of that I would want control via OSC arguments etc.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So I’m going to have fun with OSC once we’ve nailed down the requirements and in what contexts we think it could be most useful. I think the main difficulty will prove to be finding the best (and, as always, the least time-consuming) way to get the data across the internet. I feel this may best be achieved by including Agora or Normcore in the OSC project and sending the data wrapped in JSON to the main VR app. Once it has arrived, it can then be interpretted and acted upon.&lt;/p&gt;

&lt;p&gt;In a way, this will negate the need for the OSC Simple package, as there will likely be no need to broadcast via http and listen at the other end. However, it could be more flexible in the long run to use as much of the basic OSC broadcasting pattern and the technology behind it as possible, and as little as possible of the mechanism used to send the data occross the &lt;em&gt;World Wide Web&lt;/em&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">OSC Controller</summary></entry></feed>