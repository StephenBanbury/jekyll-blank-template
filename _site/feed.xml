<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-10-28T17:13:39+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Creative Reflective Journal</title><subtitle>My Creative Reflective Journal towards the Falmouth University Creative App Development Masters Degree.</subtitle><entry><title type="html">Multispace 4</title><link href="http://localhost:4000/2020/10/27/multi-space-4.html" rel="alternate" type="text/html" title="Multispace 4" /><published>2020-10-27T00:00:00+00:00</published><updated>2020-10-27T00:00:00+00:00</updated><id>http://localhost:4000/2020/10/27/multi-space-4</id><content type="html" xml:base="http://localhost:4000/2020/10/27/multi-space-4.html">&lt;h1 id=&quot;problems-with-teleporting-the-player&quot;&gt;Problems with teleporting the player&lt;/h1&gt;

&lt;p&gt;I have been having great difficulty with the function of teleportation of the Oculus PlayerController from one space/scene to another.&lt;/p&gt;

&lt;p&gt;I would have thought that would be reasonably straightforward, in the same way as it is for spawning inanimate objects, such as screens and selection panels, by simply changing the transform associated with the player. I have set up spawn points and placed these within the individual scenes, then attempted to take their Vector3 positions and applied them to the player’s. However, I end up moving in an outrageous manner, often ending up outside the area of the scenes, or even off the floor panel altogether, leading the player to end up in freefall.&lt;/p&gt;

&lt;p&gt;I have attempted placing the player within an empty GameObject, as well as stand-alone, but the same result ensues.&lt;/p&gt;

&lt;p&gt;This has been immensely frustrating.&lt;/p&gt;

&lt;h1 id=&quot;figured-out-the-issue-to-an-extent-but-do-not-have-a-solution&quot;&gt;Figured out the issue (to an extent) but do not have a solution&lt;/h1&gt;

&lt;p&gt;I set up a marker - a red capsule - and instead of applying the spawn point positions to the Player, I applied them to the marker. The marker moved around the environment perfectly and as should be expected! So the problem is with the player.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spawn the player&lt;/strong&gt;&lt;br /&gt;
&lt;img src=&quot;\images\GAM750\spawnpoint-method1.JPG&quot; alt=&quot;SpawnPlayer&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is what it looks like in the environment with scenes. You can that sometimes the player is just shifted a few feet, other times the player is moved much further, but nowhere near the spawn point, and even off the floor plane.&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\GAM750\spawnplayer-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;Here you can see quite clearly the layout of the environment with no screens to obscure it&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\GAM750\spawnplayer-2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Spawn the marker&lt;/strong&gt;&lt;br /&gt;
&lt;img src=&quot;\images\GAM750\spawnpoint-method2.JPG&quot; alt=&quot;SpawnMarker&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here you can the red marker - which is a simple 3D object with no VR special characteristics - move and spawn as expected&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\GAM750\spawnmarker-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;I experimented with using the Normcore VR Player, which is simply an avatar with the Normcore Realtime components attached to allow it to work in multiplayer, and is cloned during game-play, and the ‘regular’, local player, that contains the camera rig (so naturally, that would appear to be one to use).&lt;/p&gt;

&lt;p&gt;I also deactivated the Normcore-enabled player / avatar set-up altogether, and tried out a simple, basic, vanilla Oculus OVR PlayerController. This had exactly the same effect.&lt;/p&gt;

&lt;p&gt;To illustrate what is going on: -&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The spawnpoint’s location, where the player is supposed to be positioned (0, 1, 20)&lt;/strong&gt;
&lt;img src=&quot;\images\GAM750\spawnpoint-error-inspector-2.JPG&quot; alt=&quot;SpawnMarker&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The values being applied to the player’s transform (-0.8, 0.4, 20.2) - near enough to the spawnpoint’s exact location&lt;/strong&gt;&lt;br /&gt;
&lt;img src=&quot;\images\GAM750\spawnpoint-error-console-1.JPG&quot; alt=&quot;SpawnMarker&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The location the player actually ends up! (39.8, 0, -20.8). Go figure!&lt;/strong&gt;&lt;br /&gt;
&lt;img src=&quot;\images\GAM750\spawnpoint-error-inspector-1.JPG&quot; alt=&quot;SpawnMarker&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;The x coordinate is way, way out! How can this be?!!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It’s difficult to know where to go next with this! I wonder if it would be worth considering attaching the player to another object and using motion to move it to another space - as simple repositioning of a containing object had the same erroneous effect.&lt;/p&gt;

&lt;p&gt;First port of call, though, will be to research, to try to find something about this online.&lt;/p&gt;

&lt;p&gt;As The Terminator once famously said, &lt;strong&gt;I’ll be back…&lt;/strong&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">Problems with teleporting the player</summary></entry><entry><title type="html">Meet Me On The Other Side</title><link href="http://localhost:4000/2020/10/25/meet-me-on-the-other-side.html" rel="alternate" type="text/html" title="Meet Me On The Other Side" /><published>2020-10-25T00:00:00+01:00</published><updated>2020-10-25T00:00:00+01:00</updated><id>http://localhost:4000/2020/10/25/meet-me-on-the-other-side</id><content type="html" xml:base="http://localhost:4000/2020/10/25/meet-me-on-the-other-side.html">&lt;h1 id=&quot;pervasive-media-studio&quot;&gt;Pervasive Media Studio&lt;/h1&gt;

&lt;p&gt;The Pervasive Media Studio is a partnership between Watershed, UWE Bristol and University of Bristol.&lt;/p&gt;

&lt;p&gt;https://www.youtube.com/watch?v=yt0Hnxz31V0&lt;/p&gt;

&lt;p&gt;Interesting talk about VR, immersion, VR experience&lt;/p&gt;

&lt;p&gt;Opposiing concepts: Cartesian Metaphysics (Descart) / Embodied Cognition&lt;/p&gt;

&lt;p&gt;Social platofmr tent to attract social groupins that match other social contexts&lt;/p&gt;

&lt;p&gt;Altspace (19:30)&lt;/p&gt;

&lt;p&gt;Hubs by Mozilla (22.30)
    no headset required
    more about creating your own personona
    more control, build from scratch
    challenging&lt;/p&gt;</content><author><name></name></author><summary type="html">Pervasive Media Studio</summary></entry><entry><title type="html">Multispace 3</title><link href="http://localhost:4000/2020/10/25/multi-space-3.html" rel="alternate" type="text/html" title="Multispace 3" /><published>2020-10-25T00:00:00+01:00</published><updated>2020-10-25T00:00:00+01:00</updated><id>http://localhost:4000/2020/10/25/multi-space-3</id><content type="html" xml:base="http://localhost:4000/2020/10/25/multi-space-3.html">&lt;h1 id=&quot;controlling-player-and-actions-within-multiple-scenes&quot;&gt;Controlling player and actions within multiple scenes&lt;/h1&gt;

&lt;p&gt;Spawning scenes, including screen formations, is just the start. How do you control the screen formations and the video projections and live video feeds for each individiual screen?&lt;/p&gt;

&lt;p&gt;One line of enquiry, and the first option I tried, instantiates the selection control panel into the same starting scene as the player. The idea would then to ensure it is instantiated in the other scenes, with the player, when the player moves into them. 
This is a method I could end up working with.&lt;/p&gt;

&lt;p&gt;Another option, and the second I am trying, is to instantiate the panel in all of the scenes (along with the audio source and lighting). The panel will probably only be visible to the player - each player can own their very own panel and show and hide it as they please (or it may end up being discarded altogether).&lt;/p&gt;

&lt;p&gt;The difficulty then, with either of these options, is to ensure that: -&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;all actions cause an effect on the specific scene in which the player is situated&lt;/li&gt;
  &lt;li&gt;all effects are permiated through to the multiplayer models and then to each player instance&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;update&quot;&gt;Update&lt;/h1&gt;

&lt;p&gt;I have managed to set the screen formation animation to occur for a specific scene when the button for that formation is tapped on the panel associated with that specific scene. This is done by finding the parent scene of the selection panel and sending it to the tweening method. Remember, the tweening, along with spawning, is already able to pinpoint which scene to act upon - this has made further development - and my life - much simpler!&lt;/p&gt;

&lt;p&gt;I am now concerned that this may be difficult to translate into the multiplayer environment. This will be another stage of development in the near future.&lt;/p&gt;</content><author><name></name></author><summary type="html">Controlling player and actions within multiple scenes</summary></entry><entry><title type="html">Multispace 2</title><link href="http://localhost:4000/2020/10/24/multi-space-2.html" rel="alternate" type="text/html" title="Multispace 2" /><published>2020-10-24T00:00:00+01:00</published><updated>2020-10-24T00:00:00+01:00</updated><id>http://localhost:4000/2020/10/24/multi-space-2</id><content type="html" xml:base="http://localhost:4000/2020/10/24/multi-space-2.html">&lt;h1 id=&quot;creating-multiple-scenes-from-an-orginal-template&quot;&gt;Creating multiple scenes from an orginal template&lt;/h1&gt;

&lt;p&gt;Having given the concept of mutlple spaces some thought, where each space represents a &lt;em&gt;scene&lt;/em&gt; in the multiplayer theatre environment, it became clear that I could make use of the existing infrastructure I have already put in place (see &lt;strong&gt;&lt;a href=&quot;/2020/09/01/creating-the-screens-1.html&quot;&gt;Creating the screens 1&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&quot;/2020/09/03/creating-the-screens-2.html&quot;&gt;Creating the screens 2&lt;/a&gt;&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;This method uses a class model for each screen formation, comprising definitions for the postition and rotation of each of the sixteen screen display panels within the respective formation. This information is then used when instantiating, or &lt;em&gt;spawning&lt;/em&gt;, the formation. Animations are created by &lt;em&gt;tweening&lt;/em&gt; between the previous formation’s positioning data and that of the next formation. This has, thus far, been happening in a single location, i.e. that set in each formation’s respective model.&lt;/p&gt;

&lt;p&gt;Taking this concept and applying it to the creating of a second (or third or fourth etc.) formation, it becomes clear that the existing definitions can be used as a template, with an &lt;em&gt;offset&lt;/em&gt; applied in order to translate the positional information from the original formation to another in another place within the world space.&lt;/p&gt;

&lt;p&gt;This approach requires a set of definitions for the locations of each scene within the world space. If the original scene is set at &lt;em&gt;Vector3(0, 0, 0)&lt;/em&gt;, then the second scene can exist at &lt;em&gt;Vector3(20, 0, 0)&lt;/em&gt;, and the third at &lt;em&gt;Vector3(20, 0, 20)&lt;/em&gt; etc. 
It is then simply a case of applying this information in order to offset the original formation’s positional data. The rotations must remain the same as we do not want the screens to change direction!
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Transversing the multispace&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\GAM750\transverse-multispace-2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scene position definitions within the world space&lt;/strong&gt;&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;\images\GAM750\GetScenePosition-1.JPG&quot; alt=&quot;GetScenePosition&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Screen formations, showing screen positions offset by selected scene postition&lt;/strong&gt;&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;\images\GAM750\ScreenFormationService-1.JPG&quot; alt=&quot;ScreenFormationService&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Scene detail model, showing how each scene comprises a scene position in world space, a screen formation definition, and references to the actual screens that have been instantiated&lt;/strong&gt;&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;\images\GAM750\SceneDetailModel.JPG&quot; alt=&quot;SceneDetailModel&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Screen position model for each individual screen&lt;/strong&gt;&lt;br /&gt;&lt;br /&gt;
&lt;img src=&quot;\images\GAM750\ScreenFormationModel.JPG&quot; alt=&quot;SceneDetailModel&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Code behind the multispace&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\GAM750\multispace-code-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;</content><author><name></name></author><summary type="html">Creating multiple scenes from an orginal template</summary></entry><entry><title type="html">Multispace 1</title><link href="http://localhost:4000/2020/10/23/multi-space-1.html" rel="alternate" type="text/html" title="Multispace 1" /><published>2020-10-23T00:00:00+01:00</published><updated>2020-10-23T00:00:00+01:00</updated><id>http://localhost:4000/2020/10/23/multi-space-1</id><content type="html" xml:base="http://localhost:4000/2020/10/23/multi-space-1.html">&lt;p&gt;&lt;img src=&quot;\images\GAM750\lightbulb.jpg&quot; alt=&quot;Lightbulb&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;thinking-about-multiple-spaces-or-multiple-universes&quot;&gt;Thinking about multiple spaces (or multiple universes)&lt;/h1&gt;

&lt;p&gt;I’ve just had an interesting thought, as a respone to the problem of creating multiple &lt;em&gt;‘spaces’&lt;/em&gt;, which players can move between, at times that will be decided later.&lt;/p&gt;

&lt;p&gt;If an &lt;em&gt;‘environment’&lt;/em&gt; can be spawned as an object, and accessed through a set of methods and properties etc., then this will greatly ease the way in which a player can be ‘dropped’ into the space and interect with the objects that are present within it.&lt;/p&gt;

&lt;p&gt;In a way, this is similar to creating multiple &lt;em&gt;scenes&lt;/em&gt; in Unity, except that these exist as entities within the same single scene.  In this form, therefore, they can exploit an outward facing &lt;em&gt;interface&lt;/em&gt; in order to interact with a Master Game Controller-type object, that controls the manner of who, what, why, and where a player can exist and function, but only within the specific space that it exists.&lt;/p&gt;

&lt;h1 id=&quot;nice-theory-what-about-the-actualities&quot;&gt;Nice theory, what about the actualities?&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Negative: Could ge difficult at this stage of development, having already put much infrastructure into place.&lt;/li&gt;
  &lt;li&gt;Positive: It is actually fairly early in development terms given a longer view on the project as a whole and its potential as we move forward&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It would be a task that would include creating an interface for the space, in order to access multiple other interfaces, i.e. per method an property that is part of that space.&lt;/p&gt;

&lt;h1 id=&quot;but-no&quot;&gt;But… no!&lt;/h1&gt;

&lt;p&gt;This idea would not work - I think - because the VR experience is seen from a &lt;strong&gt;first person&lt;/strong&gt; perspective, and all code is run in a similar manner. That is, each individual instance of Unity, running in each headset, will access its own set of game objects and, on each object, a set of components. That individual is &lt;strong&gt;not&lt;/strong&gt; controlling from heaven, but from their own person… on the ground.  The &lt;em&gt;‘God’&lt;/em&gt; concept suggested above, i.e. where control is exerted down on the multispace and controlling eacn individual multiplace, could work as part of a controller interface for the use of &lt;strong&gt;Stage Managers&lt;/strong&gt; and similar roles. But this is looking too complex for this purpose and within the available time frame.&lt;/p&gt;

&lt;p&gt;Therefore, I will maintain this post as a reminder of a possible way to think of this Potential model in the future.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Spatial Audio With Normcore</title><link href="http://localhost:4000/2020/10/18/spatial-audio-with-normcore.html" rel="alternate" type="text/html" title="Spatial Audio With Normcore" /><published>2020-10-18T01:00:00+01:00</published><updated>2020-10-18T01:00:00+01:00</updated><id>http://localhost:4000/2020/10/18/spatial-audio-with-normcore</id><content type="html" xml:base="http://localhost:4000/2020/10/18/spatial-audio-with-normcore.html">&lt;h1 id=&quot;out-of-the-box-audio-appears-to-be-2d&quot;&gt;Out-of-the-box: audio appears to be 2D!&lt;/h1&gt;
&lt;p&gt;During the first realtime testing with the project’s expert stakeholders, i.e. the director and technical director, I was eager to see if audio between players is 3D / spatialised, i.e. can be used to sense distance and direction. Unfortunately, it appeared to be very much 2D and unspatialised; there was no volume roll-off - no matter how near or far apart we were from each other, the volume of the voice was constant.&lt;/p&gt;

&lt;p&gt;This is a real shame.&lt;/p&gt;

&lt;p&gt;I am now looking into possible solutions for this. I am hoping that is will be as straightforward as a simple setting somewhere, or a case of adding a 3D sound source to the VR player.&lt;/p&gt;

&lt;h1 id=&quot;testing-two-headsets-are-better-than-one&quot;&gt;Testing: two headsets are better than one&lt;/h1&gt;
&lt;p&gt;Question: How do you test this with only a single headset? 
Answer: With great difficulty!&lt;/p&gt;

&lt;p&gt;Testing multiplayer environments is not the only reason I bought a new Oculus Quest 2, but it is the main one. Or, at least, it’s a good excuse!&lt;/p&gt;

&lt;p&gt;Not only can I now experience multiplayer applicaitons with family and friends, but I can also test my own developments. Not to mention that the latest model is much faster and has a much better display, at a cheaper price point. Seemed to make sense to me!&lt;/p&gt;

&lt;p&gt;Setting up is a bit of a problem, though, because Facebook now insist that you use a FB account! It is not even possible just to use your old Oculus account - it now must be linked to a FB account, even if you continue to use it only on your current headset. Bad, bad Facebook! This means that, in order to play multiplayer, &lt;strong&gt;I need to sign in on different accounts&lt;/strong&gt;. For this reason, I set up a second FB account attached to a second user on my Android phone. I will have to log into each, separately, and use each account to log into each headset, separately. What a faff!&lt;/p&gt;

&lt;p&gt;Back to the spacial audio issue…&lt;/p&gt;

&lt;h1 id=&quot;first-port-of-call-normcore-discord-server&quot;&gt;First port of call: Normcore Discord server&lt;/h1&gt;

&lt;p&gt;[01-05-2019]&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;if you’re using the built-in spatializer, then you can fix easily by changing this line in AudioOutput.cs:&lt;/p&gt;

  &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;data[sOut] = !_mute ? audioData[sIn] : 0.0f;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;just change that to this:&lt;/p&gt;

  &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;data[sOut] *= !_mute ? audioData[sIn] : 0.0f;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;the input value to that OnAudioFilterRead() is set up to be 1.0 * whatever attenuation Unity has added to the AudioSource including volume&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;[01-09-2020]&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;I think in V1 I had disabled the built-in spatialization&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;[02-09-2020]&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Question: How do I make the player voice 3D? (e.g. how do i make them louder the closer you are to them, and quieter when you are further away? - by default you can hear people constantly)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;[02-09-2020]&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;take a look at the audio source on your avatar (make sure the “spatial” slider is set to “3d”) and you can muck with the falloff values on there if you want to customize it.
Unity defaults to 2D, and the falloff range is basically “100% volume up to 5 meters, still audible 100 meters away” or something like that, so it’s definitely tuned for larger spaces.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;[02-09-2020]&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;if you add an audio source RealtimeAvatarVoice will detect it and use it instead of creating one
you can use SetActive, but you’ll also need to make a custom RealtimeComponent in order to synchronize the active state. Normcore doesn’t synchronize that part for you&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;[04-09-2020]&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;I followed your advice to add an audio source to the RealtimeAvatarVoice gameobject in order to allow the spatial blend of the player’s voice to be 3d, however its still behaving like its 2D - any idea on what could be wrong?
general/04-09-2020&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;[04-09-2020]&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;if you’re on V1, try making this change:
&lt;a href=&quot;if you’re on V1, try making this change:
https://discord.com/channels/393839515074297858/393841777091543052/573241867185946680&quot;&gt;if you’re on V1, try making this change:
https://discord.com/channels/393839515074297858/393841777091543052/573241867185946680
&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name></name></author><summary type="html">Out-of-the-box: audio appears to be 2D! During the first realtime testing with the project’s expert stakeholders, i.e. the director and technical director, I was eager to see if audio between players is 3D / spatialised, i.e. can be used to sense distance and direction. Unfortunately, it appeared to be very much 2D and unspatialised; there was no volume roll-off - no matter how near or far apart we were from each other, the volume of the voice was constant.</summary></entry><entry><title type="html">Building The Apps</title><link href="http://localhost:4000/2020/10/01/building-the-apps.html" rel="alternate" type="text/html" title="Building The Apps" /><published>2020-10-01T03:00:00+01:00</published><updated>2020-10-01T03:00:00+01:00</updated><id>http://localhost:4000/2020/10/01/building-the-apps</id><content type="html" xml:base="http://localhost:4000/2020/10/01/building-the-apps.html">&lt;h1 id=&quot;audience---oculus-quest&quot;&gt;Audience - Oculus Quest&lt;/h1&gt;

&lt;h1 id=&quot;performer-main-room---pcmac&quot;&gt;Performer Main Room - PC/Mac&lt;/h1&gt;

&lt;p&gt;PC build works. 
Mac build creates exceptions - I believe these can be traced to requirements for the &lt;strong&gt;info.plist&lt;/strong&gt; - i.e. requesting permissions&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Microphone class is used but Microphone Usage Description is empty. App will not work on macOS 10.14+.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Search leads to responses such as &lt;a href=&quot;https://answers.unity.com/questions/1735434/build-error-microphone-class-is-used-but-microphon.html&quot;&gt;You need to add a microphone usage description string in iOS Player Settings (which will be added to your Info.plist)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://developer.apple.com/documentation/avfoundation/cameras_and_media_capture/requesting_authorization_for_media_capture_on_macos?language=objc&quot;&gt;Requesting Authorization for Media Capture on macOS&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I need the mike, so this is a requirement.&lt;/p&gt;

&lt;p&gt;I found all files named info.plist and added the line, where it was missing:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;key&gt;NSMicrophoneUsageDescription&lt;/key&gt;
  &lt;string&gt;Yes&lt;/string&gt;
&lt;/blockquote&gt;

&lt;p&gt;However, this did not work - I still get the microphone error.&lt;/p&gt;

&lt;h1 id=&quot;performer-video-streaming---pcmac&quot;&gt;Performer Video Streaming - PC/Mac&lt;/h1&gt;

&lt;h1 id=&quot;stage-manager-osc-controller---pcmac&quot;&gt;Stage Manager OSC Controller - PC/Mac&lt;/h1&gt;

&lt;p&gt;The PC apps need to be built for Mac - it seems the world of Theatre runs on Macs!&lt;/p&gt;</content><author><name></name></author><summary type="html">Audience - Oculus Quest</summary></entry><entry><title type="html">Screen Panel Trigger Button</title><link href="http://localhost:4000/2020/10/01/screen-panel-trigger-button.html" rel="alternate" type="text/html" title="Screen Panel Trigger Button" /><published>2020-10-01T02:00:00+01:00</published><updated>2020-10-01T02:00:00+01:00</updated><id>http://localhost:4000/2020/10/01/screen-panel-trigger-button</id><content type="html" xml:base="http://localhost:4000/2020/10/01/screen-panel-trigger-button.html">&lt;h1 id=&quot;the-screen-is-now-a-button&quot;&gt;The screen is now a ‘button’&lt;/h1&gt;

&lt;p&gt;When the player’s hand touches the screen, one of two actions happen, randomly: -&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A randomly selected video clip plays&lt;/li&gt;
  &lt;li&gt;An animation occurs into a new randomly selected screen formation&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This can be fairly startling, particularly the sudden change to screen formation, even more so when uninitiated and unexpected.&lt;/p&gt;

&lt;p&gt;The number and type of different actions can be added to as development continues.&lt;/p&gt;

&lt;p&gt;One idea would be to have some control of which of the actions happens, for example, by specific screen or by specific players. The latter would require identification of individual players, or of &lt;em&gt;‘type’&lt;/em&gt; of player, in the first place - again, something that can be added in a future development.&lt;/p&gt;</content><author><name></name></author><summary type="html">The screen is now a ‘button’</summary></entry><entry><title type="html">The Performer App 2</title><link href="http://localhost:4000/2020/10/01/the-performer-app-2.html" rel="alternate" type="text/html" title="The Performer App 2" /><published>2020-10-01T01:00:00+01:00</published><updated>2020-10-01T01:00:00+01:00</updated><id>http://localhost:4000/2020/10/01/the-performer-app-2</id><content type="html" xml:base="http://localhost:4000/2020/10/01/the-performer-app-2.html">&lt;h1 id=&quot;problem-solved&quot;&gt;Problem solved&lt;/h1&gt;

&lt;p&gt;I created a copy of the main VR scene and reset the UUID of the Normcore Realitime View (Script) attached to the MediaDisplayManager GameObject.&lt;/p&gt;

&lt;p&gt;The MediaDisplayManager exists to house the management classes that control media-related objects and actions, such as the screen panels, e.g. video screen selection and media assignment, the Agora video streaming controller and interface, and other related management tasks - simply, a kind of Game Manager but specifically for controlling media in the widest sense.&lt;/p&gt;

&lt;p&gt;I then replaced the VR player with another, exactly the same but using an ‘invisible’ copy of the OVR player, i.e. I disabled the body and just kept the camera and Realtime components to persist the player in the multiplayer environment. I added a Rigidbody and a script to allow control via the keyboard rather than VR.&lt;/p&gt;

&lt;p&gt;When this scene is built, it must have the VR Plugin Management disabled in Player Settings. This then creates an exact copy of the VR scene, in the multiplayer environment, but the player is invisible and unable to affect any change or force on the space - a ghost. When a PC build is created it runs on another PC - the VR character(s) in the space can be viewed along with the results of their actions, i.e. screen/video/formation selection and assignment changes etc., whilst the invisible player can move around using the keyboard.&lt;/p&gt;</content><author><name></name></author><summary type="html">Problem solved</summary></entry><entry><title type="html">The Performer App 1</title><link href="http://localhost:4000/2020/09/27/the-performer-app-1.html" rel="alternate" type="text/html" title="The Performer App 1" /><published>2020-09-27T01:00:00+01:00</published><updated>2020-09-27T01:00:00+01:00</updated><id>http://localhost:4000/2020/09/27/the-performer-app-1</id><content type="html" xml:base="http://localhost:4000/2020/09/27/the-performer-app-1.html">&lt;h1 id=&quot;how-should-the-performer-experience-the-vr-space&quot;&gt;How should the performer experience the VR space?&lt;/h1&gt;
&lt;p&gt;I had been wondering for some time how I would approach the issue of enabling the performers, who are streaming live video into the VR space to be projected onto the panel screens, to &lt;strong&gt;also be able to see what is happening within the space&lt;/strong&gt;, where audience members are located, where they are looking, how they are moving; the screen formations and how they morph into different shapes; lighting; other performers and where they are beging displayed etc.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rendering video via WebRTC&lt;/strong&gt;&lt;br /&gt;
I had assumed that I would need to stream video, or &lt;em&gt;volumetric capture&lt;/em&gt;, from the environment to be displayed, for example, on a browser or some other display. I imagined this would make use of WebRTC in a similar way as streaming live video of a performer into the VR space. I’m sure it would be possible, but it would be time consuming - not something I would relish after having already spent a significant amount of effort and time on the stack put in place thus far. However, it would certainly be an option for further development if required - Oculus has developed a &lt;strong&gt;&lt;a href=&quot;https://forum.unity.com/threads/unity-render-streaming-introduction-faq.742481/&quot;&gt;streaming solution&lt;/a&gt;&lt;/strong&gt; (the GitHub repository is &lt;strong&gt;&lt;a href=&quot;https://github.com/Unity-Technologies/UnityRenderStreaming&quot;&gt;here&lt;/a&gt;)&lt;/strong&gt;, so it’s not unprecedented.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sending positional data to the performer/client&lt;/strong&gt;&lt;br /&gt;
Then I began to think of streaming pure location data for performers and screens etc to a separate app, where they could be used to map the space in real time.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Joining the performer into the multiplayer space&lt;/strong&gt;&lt;br /&gt;
However, the most immediately straightforward option would also seem to be potentially the most useful: to join the performer into the multiplayer space as an invisible player, who can roam around with absolute freedom, but who’s presence cannot be perceived by the audience. The performer would not be experiencing the space through VR, but on a 2D flat screen, and controlling their movements using keyboard or mouse. I did briefly experiment with just having an overhead camera above the space, but this was soon replaced with the more immersive option of being in the space itself.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Overhead Camera&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\GAM750\overhead-camera-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Performer ‘in space’&lt;/strong&gt;&lt;br /&gt;
Here the performer is in the space with a single VR player (i.e. audience). This is what it could look like when it’s working - however, I am currently having some issues with Normcore allowing this to happen - it sometimes crashes out, loses connection, or freezes for the performer app. Sometimes it just does not follow actions from the VR environment. It may be that there is not enough separation, somehow, between the player, their Normcore ‘realtime’ player, their Oculus avatar.. or at least somewhere in there there is an issue - tracking it is proving difficult. But the video below at least shows what could be possible!&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\GAM750\performer-space-2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;And here’s an example of an ‘error’&lt;/strong&gt;&lt;br /&gt;
  ..the actions on the left (i.e. VR), are not propergated on the right (non-VR)&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\GAM750\performer-space-err-2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;</content><author><name></name></author><summary type="html">How should the performer experience the VR space? I had been wondering for some time how I would approach the issue of enabling the performers, who are streaming live video into the VR space to be projected onto the panel screens, to also be able to see what is happening within the space, where audience members are located, where they are looking, how they are moving; the screen formations and how they morph into different shapes; lighting; other performers and where they are beging displayed etc.</summary></entry></feed>