<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-09-07T22:18:11+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Creative Reflective Journal</title><subtitle>My Creative Reflective Journal towards the Falmouth University Creative App Development Masters Degree.</subtitle><entry><title type="html">Creating The Screens 2</title><link href="http://localhost:4000/2020/09/03/creating-the-screens-2.html" rel="alternate" type="text/html" title="Creating The Screens 2" /><published>2020-09-03T01:00:00+01:00</published><updated>2020-09-03T01:00:00+01:00</updated><id>http://localhost:4000/2020/09/03/creating-the-screens-2</id><content type="html" xml:base="http://localhost:4000/2020/09/03/creating-the-screens-2.html">&lt;h1 id=&quot;animations&quot;&gt;Animations&lt;/h1&gt;

&lt;p&gt;I came across an interesting-looking Unity animation asset - &lt;strong&gt;&lt;a href=&quot;http://dotween.demigiant.com/&quot;&gt;DOTween&lt;/a&gt;&lt;/strong&gt; - whilst working on the ShelterBox project. In fact, so interesting it looked I purchased the ‘pro’ version - &lt;strong&gt;&lt;a href=&quot;http://dotween.demigiant.com/pro.php&quot;&gt;DOTween Pro&lt;/a&gt;&lt;/strong&gt; - before even using it; I had a feeling it would come in handy in future projects even if I didn’t get to use it at the time. And, sure enough, this has been the case.&lt;/p&gt;

&lt;p&gt;DOTween has many features, but the main attration for me was the way it allows comlex tweening and manipulation of animations via code and, as such, can be controlled within the codebase of the project without having to resort to the somewhat ‘clunky’ (in my opinion) animation creation tools available out of Unity the box.&lt;/p&gt;

&lt;p&gt;I have used it here in order to tween between one set of &lt;em&gt;Vector3&lt;/em&gt; and &lt;em&gt;Rotation&lt;/em&gt; data for a formation of screens and another. So it turns out my method of recording the coordinates data for each formation, as outlined in my previous post, was &lt;strong&gt;the correct thing to do!&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\screenanimations-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\screenanimations-2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;creating-the-formations&quot;&gt;Creating the formations&lt;/h1&gt;

&lt;p&gt;Creating the actual formations involved much laborious moving and positioning of screens in the Unity editor, as well as some &lt;strong&gt;calculating&lt;/strong&gt; of coordinates in order to create specific shapes. For example, the &lt;strong&gt;isosceles triangle&lt;/strong&gt; required the three angles to be calculated depending on the lengths of the sides. As there are 16 screens, the triangle ended up having two sides comprising six screens and one side of four. To save me time, I used an &lt;strong&gt;&lt;a href=&quot;https://www.triangle-calculator.com/?what=iso&quot;&gt;online calculation tool&lt;/a&gt;&lt;/strong&gt;. The Angles of the points of the triangle were therefore calculated thus:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\images\GAM750\calc-isoscelese-triangle.JPG&quot; alt=&quot;Isoscelese triangle calculation&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">Animations</summary></entry><entry><title type="html">Creating The Screens 1</title><link href="http://localhost:4000/2020/09/01/creating-the-screens-1.html" rel="alternate" type="text/html" title="Creating The Screens 1" /><published>2020-09-01T01:00:00+01:00</published><updated>2020-09-01T01:00:00+01:00</updated><id>http://localhost:4000/2020/09/01/creating-the-screens-1</id><content type="html" xml:base="http://localhost:4000/2020/09/01/creating-the-screens-1.html">&lt;h1 id=&quot;the-screen-object&quot;&gt;The screen object&lt;/h1&gt;

&lt;p&gt;Initial prototypes were based on simplicity and guess-work. In fact they were too small and not quite the correct dimension. They also had no detail but were, instead, simple slabs.&lt;/p&gt;

&lt;p&gt;Dimensions were later provided for the screen alone and for its frame, along with resourses including model diagrams, sketches and a SketchUp model. The latter I have not been able to import as it is created within a newer version of SketchUp than mine - but I will see if there is a way.&lt;/p&gt;

&lt;p&gt;There is a very specific visual identity for the screen, which the director is keen to use. The physical screens have frames which are connected and hinged, allowing them to be folded and maneouvred, in order to create spaces that can morph into differ in shapes and sizes.&lt;/p&gt;

&lt;p&gt;The screen material itself is designed for projections. However, in my case, I needed to include a video player for local video material and a canvas onto which an Agora Surface can be created. Both of these are attached to the containing screen object as children and can be switched on and off as required.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Single screen&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\single-screen-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Multiple screens&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\multiple-screens-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;formation&quot;&gt;Formation&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Simple formations&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\screen-formations-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;Formations need to be able to morph, from one formation to another. I have been considering how this should best be done: -&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Unity physics&lt;/strong&gt;: in this scenario each frame would be hinged and when one is physically moved by a virtual presense in the space or other means, then the screens attached will also be affected. This seems a nice-to-have feature, but could be fraught with difficulties, particularly in the short space of time available.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Plotting the locations&lt;/strong&gt; of the screens on the ground, and then moving the points over time, dragging the screens into new positions, creating new formations. This would appear to provide a lot of control over where and how the screens move, but could be time-consuming.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Unity animations&lt;/strong&gt;: this could a relatively straighforward option. Would this be for individual screens - all 16 of them - or for a formation as a whole? This would be a major question when formulating a solution.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Other animation methods&lt;/strong&gt;: using DoTween or similar could be an interesting way forward. However, I have not used the asset yet, although I did pay for the Pro version some months back. DoTween looks a good package, but I’d need to study and experiment with it first.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;spawning-screens&quot;&gt;Spawning screens&lt;/h1&gt;

&lt;p&gt;I created a &lt;strong&gt;class model&lt;/strong&gt; for screen position that holds the &lt;em&gt;Vector3&lt;/em&gt; coordinates and rotation data, in degrees, and a &lt;strong&gt;service&lt;/strong&gt; to provide lists of position data for each screen in a formation.&lt;/p&gt;

&lt;p&gt;This data is then read and assigned to each screen as it is instantiated into the space. Calling the method easily allows each formation to be immediately created. Should I end up using such data to move between two points (e.g. x1 =&amp;gt; x2, y1 =&amp;gt; y2, z1 =&amp;gt; z2) then this will be very useful. Even if this is not the final method I use, it is very useful to hold this data, as a record, in order to recreate formations whenever I need to, whether this is during runtime or during development and editing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example formation with associated coordinates&lt;/strong&gt;
&lt;img src=&quot;\images\GAM750\formation-1.jpg&quot; alt=&quot;Screen Formation Image&quot; /&gt;
&lt;img src=&quot;\images\GAM750\formation-1-coordinates.jpg&quot; alt=&quot;Screen Formation Image Coordinates&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">The screen object</summary></entry><entry><title type="html">Visuals</title><link href="http://localhost:4000/2020/08/31/visuals.html" rel="alternate" type="text/html" title="Visuals" /><published>2020-08-31T01:00:00+01:00</published><updated>2020-08-31T01:00:00+01:00</updated><id>http://localhost:4000/2020/08/31/visuals</id><content type="html" xml:base="http://localhost:4000/2020/08/31/visuals.html">&lt;h1 id=&quot;why&quot;&gt;Why?&lt;/h1&gt;

&lt;p&gt;At this stage, I felt it important to spend some time working on the visual aspects of the project. The reason for this was that, as much as anything else, I’m sure the &lt;em&gt;Director/Designer&lt;/em&gt; and &lt;em&gt;Technical Director&lt;/em&gt; would appreciate seeing something that is less prototype-like, less of a &lt;em&gt;MVP&lt;/em&gt; (minimal viable product), and with the capacity to provide a little more of a sense of potential, of what the space could look like and how it can be experienced. This would provide food for the imagination, which could only serve to feed the creative and collaborative process across the team as a whole.&lt;/p&gt;

&lt;p&gt;My graphic-creating, shaping, modeling and illustration skills are not something I’ve worked on to any great extent. I have dabbled, particularly with the ShelterBox project, with fairly simple shapes creating the environment, brought to life through animation, thoughful colouring and lighting. But I have not come close to mastering the visual arts of creating something visually unique, interesting stylistically or aesthetically. It’s fair to say I’ve done as much as I have been able to given time restraints and the more pressing need to create something that works well and looks a cut above &lt;em&gt;‘good enough’&lt;/em&gt;!&lt;/p&gt;

&lt;p&gt;Two areas that could be disgnated for such attention are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;The screens&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;The selector panels&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Why?</summary></entry><entry><title type="html">OSC (Open Sound Control)</title><link href="http://localhost:4000/2020/08/19/osc.html" rel="alternate" type="text/html" title="OSC (Open Sound Control)" /><published>2020-08-19T01:00:00+01:00</published><updated>2020-08-19T01:00:00+01:00</updated><id>http://localhost:4000/2020/08/19/osc</id><content type="html" xml:base="http://localhost:4000/2020/08/19/osc.html">&lt;h1 id=&quot;what-is-osc&quot;&gt;What is OSC?&lt;/h1&gt;

&lt;p&gt;A requirement of the &lt;strong&gt;&lt;em&gt;Not Near Enough&lt;/em&gt;&lt;/strong&gt; brief is to explore the possibility of &lt;em&gt;‘external control of the virtual theatre space’&lt;/em&gt;. &lt;strong&gt;QLab&lt;/strong&gt; is mentioned, as is &lt;strong&gt;OSC&lt;/strong&gt;. Neither of these meant anything to me at the start of this project. I have begun examining OSC, as it appears to be the protocol of choice for theatre equipment interaction.&lt;/p&gt;

&lt;p&gt;A series of blogs by &lt;strong&gt;Sam Smallman&lt;/strong&gt; - a fellow Creative App Developer on the Falmouth MA course - explains OSC: -&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.etcconnect.com/2017/08/exploring-the-network-osc-part-i/&quot;&gt;Exploring the Network: The Postcard (OSC)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.etcconnect.com/2017/11/exploring-the-network-the-address/&quot;&gt;Exploring the Network: The Address&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.etcconnect.com/2018/01/exploring-the-network-the-postman/&quot;&gt;Exploring the Network: The Postman&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.etcconnect.com/2019/02/exploring-the-network-osc-part-ii/&quot;&gt;Exploring the Network: OSC Connections&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the case of this project, OSC could be used for theatre technical crew to control, for example, the selection of video clips or available video streams, and their subsequent assignment to a particular selection of screens. It could also be used to control lighting, animations, movement of screens and so on. I also imagine it will be able to be used, potentially, in controlling sound, in terms of volume or routing.&lt;/p&gt;

&lt;p&gt;The main question appears to be, how will it work in a distributed, virtual theatre environment? 
Most theatres would be using a closed network and would not need to access the internet in order for equipment to talk to each other. In our case, the very nature of the virtual theatre space is distributed, by definition. All signals will need to be directed across the internet.&lt;/p&gt;

&lt;p&gt;It also follows that, if a technician is controlling the environment, they would need visual access to it, not least to see how it is being affected by their actions. How this can made to happen is a major question that will need to be thought through.&lt;/p&gt;

&lt;p&gt;My early thoughts on this are that any OSC-outputting equipment would need to connect to a local client, which would, in turn, be connected into the multiplayer space (i.e. via Normcore). Then, any changes detected by the local client would be treated in exactly the same way as changes currently are.  Normcore will facilitate changes to values being passed on to all other clients connected into the multiplayer space.&lt;/p&gt;

&lt;p&gt;This is getting a little more complicated by the day. I feel a diagram coming on!&lt;/p&gt;</content><author><name></name></author><summary type="html">What is OSC?</summary></entry><entry><title type="html">Using Agora</title><link href="http://localhost:4000/2020/08/18/using-agora.html" rel="alternate" type="text/html" title="Using Agora" /><published>2020-08-18T01:00:00+01:00</published><updated>2020-08-18T01:00:00+01:00</updated><id>http://localhost:4000/2020/08/18/using-agora</id><content type="html" xml:base="http://localhost:4000/2020/08/18/using-agora.html">&lt;h1 id=&quot;progress-update&quot;&gt;Progress update&lt;/h1&gt;
&lt;p&gt;I have not posted for more than a week now, and this is due to the fact that nearly all my dev time (there has been family time too, or I’ll end up with only dev time in my life - not a great prospect!) has been getting to know Agora and adapting the slightly haphazard demo material for use in my own VR app. There have been frustrating moments where things just do not work as you’d hope, but there has never really been a moment when I’ve thought “this is just never going to work”, or “if this is going to work, then it’s going to be more hassle than it’s worth!”. But I have continually been aware that it may be a laborious process, and this has been the case.&lt;/p&gt;

&lt;h1 id=&quot;what-just-streaming&quot;&gt;What?! Just streaming?!&lt;/h1&gt;

&lt;p&gt;I initially set up screens to which I allowed the attaching of Agora’s &lt;em&gt;VideoSurface&lt;/em&gt; component class. Once this was working, I needed to add back in the ability to diplay video clips. To this end I have a general ‘media’ class that covers both (and will cover any new) media types. Each screen object has attached, as a child, the already-existing screen object, that includes a VideoPlayer for video clips, and now, also, a canvas, to which an Agora VideoSurface can be dynamically attached as and when a stream is created and assigned to the display.&lt;/p&gt;

&lt;p&gt;To the Normcore multiplayer model I therefore needed to add another variable representing the selected video stream. I can now select either a pre-loaded video clip or a video stream provided by an external client device’s camera, and then assign it to a specified screen.  Depending on its type, the media will be displayed as a video clip by the video player or a video stream by the Agora video surface. If either a video or a stream is selected, followed by a display screen selection, the Normcore model triggers an event that informs all other clients to also select the same media and assign it to the same display screen.&lt;/p&gt;

&lt;p&gt;Buttons to select specific video streams provided by Agora clients are dynamically created as each client joins the room. As they join, they are added to the client list (of AgoraUsers). Clients are never removed, but if they leave the room they are marked as such by a &lt;em&gt;LeftRoom&lt;/em&gt; property.  Each is assigned an ID that is used to select the specific video stream.&lt;/p&gt;

&lt;p&gt;I am using a simple client app, based on the Agora demo, to provide a portal for external clients to join the room and create a video stream. This app uses the same Agora ID as the VR app (where streams are displayed) so streams are seen by the VR app, along with all other clients.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Streaming and playing local videos to selected screens&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\streaming-video-to-different-screens-2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;problems&quot;&gt;Problems&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Currently, when a new VR app instance is created, this is viewed as a new client by the other clients, including those that are other instances of the VR app. Therefore, an Agora stream is created for each instance and appears as a new client, including a new select button, in the VR environment. I need to find a way to treat each of these as ‘dead’ streams - they should not appear at all. The VR users should only receive streams to be displayed. This may be different when the app is build for Android - I have not tested this yet - but when run as a PC app and tested via &lt;strong&gt;Oculus Link&lt;/strong&gt;, the PC’s camera is added to the stream pool.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;This has been fixed, but I’ll keep the following for the record&lt;/strong&gt;: 
I’m seeing problems where one or more of the video feeds is freezing. This does not happen straight away, but after adding more and playing with how they are routed to different screen destination. I’m not sure if this is something that is to do with streaming/bandwidth performance, or similar, or something to do with the way the Unity Agora asset works. It could be that only one live stream is allowed at a time, although my observations in a previous post would suggest that this should not be a problem:&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;The Unity asset is for ‘live interactive video streaming’. This is geared towards one single main video channel at a time streamed to multiple locations. There is another option for ‘democratic’ open channels where all can communicate at the same time to all others.&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;Of course, it could be that I have misunderstood the documentation, and the difference between &lt;strong&gt;&lt;em&gt;&lt;a href=&quot;https://docs.agora.io/en/Voice/product_voice?platform=All%20Platforms&quot;&gt;Voice Call&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;&lt;a href=&quot;https://docs.agora.io/en/Interactive%20Broadcast/product_live?platform=All%20Platforms&quot;&gt;Live Interactive Video Streaming&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;. I will be examining these descriptions closely! This is something that I very much hope is not the case, because there is no Unity asset for Void Call!&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;.. the issue turned out to be nothing of the sort and has now been fixed. I needed to ensure that any existing Agora video surfaces would be re-used if any new streams were to be displayed on its screen, i.e. replacing the existing stream. Previously, I had been destroying these existing surfaces and then creating a new one.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Streaming and playing local videos in multiplayer mode&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\streaming-video-multiplayer-3.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;</content><author><name></name></author><summary type="html">Progress update I have not posted for more than a week now, and this is due to the fact that nearly all my dev time (there has been family time too, or I’ll end up with only dev time in my life - not a great prospect!) has been getting to know Agora and adapting the slightly haphazard demo material for use in my own VR app. There have been frustrating moments where things just do not work as you’d hope, but there has never really been a moment when I’ve thought “this is just never going to work”, or “if this is going to work, then it’s going to be more hassle than it’s worth!”. But I have continually been aware that it may be a laborious process, and this has been the case.</summary></entry><entry><title type="html">Multiplayer Synchronisation</title><link href="http://localhost:4000/2020/08/07/multiplayer-syncing.html" rel="alternate" type="text/html" title="Multiplayer Synchronisation" /><published>2020-08-07T02:00:00+01:00</published><updated>2020-08-07T02:00:00+01:00</updated><id>http://localhost:4000/2020/08/07/multiplayer-syncing</id><content type="html" xml:base="http://localhost:4000/2020/08/07/multiplayer-syncing.html">&lt;h1 id=&quot;using-normcore-for-two-main-objectives&quot;&gt;&lt;strong&gt;Using Normcore for two main objectives&lt;/strong&gt;:&lt;/h1&gt;

&lt;h1 id=&quot;1-syncronising-transforms&quot;&gt;1. Syncronising transforms&lt;/h1&gt;

&lt;p&gt;Normcore includes pre-built scripts that directly synchronise GameObject transforms between clients. Very simpley put, once an object has been designated to have a &lt;em&gt;‘realtime transform’&lt;/em&gt;, it will automatically be kept in sync.&lt;/p&gt;

&lt;p&gt;The same goes for players’ avatars, which allows them to interact within the shared ‘room’.&lt;/p&gt;

&lt;p&gt;Objects can be ‘owned’ by a player, allowing them to manipulate their position or other transforms. If an object is designated to have the ability to transfer ownership, then more players within the room can also interact with it.&lt;/p&gt;

&lt;p&gt;Although this is extremely simplified, it does highlight how these concepts allow rapid development of the multiplayer environment.&lt;/p&gt;

&lt;h1 id=&quot;2-syncronising-abstract-values&quot;&gt;2. Syncronising abstract values&lt;/h1&gt;

&lt;p&gt;Translating values when values change and events are raised. In this case, selecting a video and assigning it to a display screen.&lt;/p&gt;</content><author><name></name></author><summary type="html">Using Normcore for two main objectives:</summary></entry><entry><title type="html">Video Streaming Experiments</title><link href="http://localhost:4000/2020/08/07/video-streaming-experiments.html" rel="alternate" type="text/html" title="Video Streaming Experiments" /><published>2020-08-07T01:00:00+01:00</published><updated>2020-08-07T01:00:00+01:00</updated><id>http://localhost:4000/2020/08/07/video-streaming-experiments</id><content type="html" xml:base="http://localhost:4000/2020/08/07/video-streaming-experiments.html">&lt;h1 id=&quot;early-attempts-at-setting-up-a-server&quot;&gt;Early attempts at setting up a server&lt;/h1&gt;

&lt;p&gt;As can be seen here &lt;strong&gt;&lt;a href=&quot;/2020/07/27/webrtc-web-app.html&quot;&gt;Building A WebRTC Web App&lt;/a&gt;&lt;/strong&gt;, I have build a simple &lt;strong&gt;node.js&lt;/strong&gt; server using Express. I considered hosting this, or something similar - perhaps a .NET server - in AWS or Firebase. Could I still use PeerJS as a STUN/TURN server? I’m still new to this so, even though I’m picking up new stuff contiually and gradually piecing it all together, I’m uncertain how to fit it all together. I know I can learn as I go, but it’s quite a lot to get to grips with in a short space of time - there is little scope for taking the wrong route and backin up.&lt;/p&gt;

&lt;h1 id=&quot;kurento-media-server&quot;&gt;Kurento media server&lt;/h1&gt;

&lt;p&gt;I began looking at setting up a &lt;strong&gt;&lt;a href=&quot;https://www.kurento.org/whats-kurento&quot;&gt;Kurento media server&lt;/a&gt;&lt;/strong&gt;, which could be hosted as an instance in a &lt;strong&gt;&lt;a href=&quot;https://hub.docker.com/r/kurento/kurento-media-server&quot;&gt;Docker container&lt;/a&gt;&lt;/strong&gt;. This is a super idea and is enthusiastically supported by Alcwyn Parker, the Masters Degree course leader and my research tutor on this project. However, time is, again, a major issue - for both of us. We did spend some time working on sett up such a stack, but eventually it seemed sensible to work with an out-of-the-box WebRTC solution we were aware of - &lt;strong&gt;&lt;a href=&quot;https://www.agora.io/en/&quot;&gt;Agora&lt;/a&gt;&lt;/strong&gt; - as long as research showed it to have the right potential.&lt;/p&gt;

&lt;h1 id=&quot;agora&quot;&gt;Agora&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Uses WebRTC&lt;/li&gt;
  &lt;li&gt;First 10,000 minutes free every month (so likely to remain free for the kind of use we’ll be putting it to)&lt;/li&gt;
  &lt;li&gt;Pay As You Go &lt;a href=&quot;https://www.agora.io/en/pricing/&quot;&gt;pricing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Cheap&lt;/li&gt;
  &lt;li&gt;Simple to set up&lt;/li&gt;
  &lt;li&gt;Can stream voice and video&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Unity asset contains a couple of sample scenes that are fairly easy to set up. I had some problems building the scene in order to spin up multiple instances - necessary so that multiple connections can be made - as the documentation is not 100% idiot proof, but with a bit of &lt;strong&gt;tenacity&lt;/strong&gt; and patience I got something very promising working.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\agora-test.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;The Unity asset is for ‘live interactive video streaming’. This is geared towards one single main video channel at a time streamed to multiple locations. There is another option for ‘democratic’ open channels where all can communicate at the same time to all others. I do not this the latter option will be necessary for us, because we will be streaming single performers into the Unity space.&lt;/p&gt;

&lt;p&gt;It was great to see the video images being displayed directly onto textures in Unity, such as the cube - this should prove useful and time-saving.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Agora&lt;/strong&gt; looks to be the way to go, at least for the time being. I would prefer to be building my own server and exploring the use and potential of some exciting new technologies in a more ‘raw’ form - partly for my own personal curiosity and learning, but also for an increased self-contained, stand-alone stack that does not rely so much on external products. Perhaps this could be a later development of this project once the initial objectives have been met. For now, the precious resource of time is a prime consideration.&lt;/p&gt;</content><author><name></name></author><summary type="html">Early attempts at setting up a server</summary></entry><entry><title type="html">A Little More Light Falling On WebRTC</title><link href="http://localhost:4000/2020/07/28/webrtc-2.html" rel="alternate" type="text/html" title="A Little More Light Falling On WebRTC" /><published>2020-07-28T01:00:00+01:00</published><updated>2020-07-28T01:00:00+01:00</updated><id>http://localhost:4000/2020/07/28/webrtc-2</id><content type="html" xml:base="http://localhost:4000/2020/07/28/webrtc-2.html">&lt;h1 id=&quot;jargon-buster&quot;&gt;Jargon-buster&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;NAT: Network Address Translation&lt;/li&gt;
  &lt;li&gt;SDP: Session Description Protocol&lt;/li&gt;
  &lt;li&gt;STUN: Session Traversal Utilities for NAT&lt;/li&gt;
  &lt;li&gt;TURN: Traversal Using Relay around NAT&lt;/li&gt;
  &lt;li&gt;ICE: Interactive Connectivity Establishment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As I’m new to this whole area of sockets and media streaming, it has been a quite a minefield trying to find my way around a territory that appears to offer a lot of rich resources but it tantalisingly sparcely mapped out by those who have explored it before I.&lt;/p&gt;

&lt;p&gt;So it is quite refreshing when coming across moments of lucidity from folks who clearly knows what they are talking about.  They may have been cojoled into a paragraph or two encouraging them to unearth some of the experiencial matter that has accumulated and sedimented within their minds during the time they have been working with this material, but it’s appreciated nonetheless!&lt;/p&gt;

&lt;p&gt;Enter &lt;strong&gt;&lt;a href=&quot;https://forum.unity.com/threads/unitypeerjs-simple-webrtc-support-for-unity-webgl.310166/&quot;&gt;tiggus&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Why WebRTC instead of just normal direct tcp or udp between peers?&lt;/strong&gt;
WebRTC can be initiated from a web browser specifically thus useful for HTML5. The only sockets that you can initiate from HTML5 apps are plain http, websockets(upgraded http), and WebRTC. Vanilla TCP and UDP is not an option from the browser unless you’re using plugins. You also cannot start a WebSocket listener in a browser, only outbound.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Does it provide any Nat traversal benefits?&lt;/strong&gt;
It utilizes &lt;strong&gt;STUN&lt;/strong&gt; and &lt;strong&gt;TURN&lt;/strong&gt; for &lt;strong&gt;NAT&lt;/strong&gt; traversal so you need to provide or use someone elses STUN server and also provide some sort of signalling server to do the matchmaking, but after the two peers are connected all data flows peer to peer. The bandwidth/servers required to get STUN information is provided for free by a number of providers, STUN basically just tells you your public ip address and port so you can inform the other peer who to connect to. TURN is a relay server like you would typically do with something like Photon for instance, shuttling data through a middleman server, but is optional for clients that can’t connect with plain STUN/ICE.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Next it’s &lt;strong&gt;&lt;a href=&quot;https://stackoverflow.com/questions/59484802/ice-vs-stun-vs-turn&quot;&gt;hobbs&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;TURN&lt;/strong&gt; is a relay — both clients send data to the TURN server, which forwards it to the other client.
&lt;strong&gt;STUN&lt;/strong&gt; is not a relay — the STUN server helps to “make the connection” between the clients (by discovering and exchanging their external host:port pairs), after which they send data to each other directly. However, STUN doesn’t work with all NAT/firewall setups, so TURN is used when STUN fails.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;And from &lt;strong&gt;&lt;a href=&quot;https://www.twilio.com/docs/stun-turn/faq&quot;&gt;twilio.com&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;STUN&lt;/strong&gt;, &lt;strong&gt;TURN&lt;/strong&gt;, and &lt;strong&gt;ICE&lt;/strong&gt; are a set of IETF standard protocols for negotiating traversing NATs when establishing peer-to-peer communication sessions. WebRTC and other VoIP stacks implement support for ICE to improve the reliability of IP communications.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;A host uses &lt;strong&gt;Session Traversal Utilities for NAT&lt;/strong&gt; (STUN) to discover its public IP address when it is located behind a NAT/Firewall. When this host wants to receive an incoming connection from another party, it provides this public IP address as a possible location where it can receive a connection. If the NAT/Firewall still won’t allow the two hosts to connect directly, they make a connection to a server implementing Traversal Using Relay around NAT (TURN), which will relay media between the two parties.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Interactive Connectivity Establishment&lt;/strong&gt; (ICE) is a blanket standard that describes how to coordinate STUN and TURN to make a connection between hosts. Twilio’s Network Traversal Service implements STUN and TURN for ICE-compatible clients, such as browsers supporting the WebRTC standard.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thank you &lt;strong&gt;tiggus&lt;/strong&gt;, &lt;strong&gt;hobbs&lt;/strong&gt; and &lt;strong&gt;twilio.com&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://medium.com/av-transcode/what-is-webrtc-and-how-to-setup-stun-turn-server-for-webrtc-communication-63314728b9d0&quot;&gt;Medium: What is WebRTC and How to Setup STUN/TURN Server for WebRTC Communication?&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;ICE&lt;/strong&gt; (Interactive Connectivity Establishment) is a protocol used to generate media traversal candidates that can be used in WebRTC applications, and it can be successfully sent and received through Network Address Translation (NAT)s using STUN and TURN.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;STUN&lt;/strong&gt; (Session Traversal Utilities for NAT) allows applications to discover the presence and types of NATs and firewalls between them and on the public Internet. It can be used by any device to determine the IP address and port allocated to it by a NAT.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;TURN&lt;/strong&gt; (Traversal Using Relays around NAT) Server allows clients to send and receive data through an intermediary server. The TURN protocol is the extension to STUN.&lt;br /&gt;
TURN is most useful for Web, Mobile and IoT clients on networks masqueraded by symmetric NAT devices. But the TURN server cost is high because of the server utilization and huge bandwidth usage in the case where more client connections are established.&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name></name></author><summary type="html">Jargon-buster</summary></entry><entry><title type="html">Building A WebRTC Web App</title><link href="http://localhost:4000/2020/07/27/webrtc-web-app.html" rel="alternate" type="text/html" title="Building A WebRTC Web App" /><published>2020-07-27T23:00:00+01:00</published><updated>2020-07-27T23:00:00+01:00</updated><id>http://localhost:4000/2020/07/27/webrtc-web-app</id><content type="html" xml:base="http://localhost:4000/2020/07/27/webrtc-web-app.html">&lt;p&gt;As stated in the previous post, at the moment &lt;strong&gt;I feel that my time would be best spent learning about WebRTC in general, rather than  specifically as a Unity plugin&lt;/strong&gt;. Once I feel I fully understand the basics I should be able to apply them more easily to my specific Unity test-case.&lt;/p&gt;

&lt;p&gt;I have found a useful-looking half-hour-long YouTube tutorial, &lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=DvlyzDZDEq4&quot;&gt;How To Create A Video Chat App With WebRTC&lt;/a&gt;&lt;/strong&gt;. It was only published a day ago but has already received over 2,000 ‘likes’ (and only 15 thumbs-down). If nothing else, I see this as further evidence of the current, pandemic-induced social lockdown interest in the subject of peer-to-peer video chat apps.&lt;/p&gt;

&lt;p&gt;I have also placed a tutorial into my Udemy Cart and may decide to purchase it. I have 10 hours to get it at the discount price of £13.99 (reduced fro £49.99, apparently). But first I will see what the free YouTube offering is about.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;I note the many cricisms in the review/comments section that this course is out of date, is deprecated and not been updated for three years. This may be unfair, but I think I’ll just find what I need for free on YouTube etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;notes-on-setting-up-the-webrtc-project&quot;&gt;Notes on setting up the WebRTC project&lt;/h1&gt;

&lt;p&gt;Setting up the server&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;npm i express ejs socket.io&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use the &lt;a href=&quot;https://expressjs.com/&quot;&gt;Expressjs server&lt;/a&gt;: &lt;a href=&quot;https://expressjs.com/en/starter/installing.html&quot;&gt;expressjs.com/installing&lt;/a&gt;, &lt;a href=&quot;https://www.npmjs.com/package/express&quot;&gt;npm&lt;/a&gt;,&lt;/li&gt;
  &lt;li&gt;use dependency ejs - templating language / view engine&lt;/li&gt;
  &lt;li&gt;use socket.io&lt;/li&gt;
  &lt;li&gt;uuid&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;npm i uuid&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;allows us to create dynamic IDs for different rooms&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;npm i –save-dev nodemon&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Dynamic refreshing of server on making changes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;server.js&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;const express = require ('express')
const app = express()
const server = require ('http').Server(app)
const io = require('socket.io')(server)
const { v4: uuidV4 } = require('uuid')

app.set('view engine', 'ejs')
app.use(express.static('public'))

app.get('/', (req, res) =&amp;gt; {
    res.redirect(`/${uuidV4()}`)
})

app.get('/:room', (req, res) =&amp;gt; {
    res.render('room', { roomId: req.params.room })
})

server.listen(3000)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;room.ejs&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: you can use ! in a new file to generate HTML boilerplate code.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;&amp;lt;!DOCTYPE html&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;html&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;lang=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;head&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;meta&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;charset=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;UTF-8&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;meta&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;viewport&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;content=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;width=device-width, initial-scale=1.0&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;title&amp;gt;&lt;/span&gt;Document&lt;span class=&quot;nt&quot;&gt;&amp;lt;/title&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;style&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nf&quot;&gt;#video-grid&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;nl&quot;&gt;display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;py&quot;&gt;grid-template-columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;repeat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;auto-fill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;300px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
            &lt;span class=&quot;py&quot;&gt;grid-template-rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;300px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;video&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;nl&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;100%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;nl&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;100%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;nl&quot;&gt;object-fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cover&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;/style&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/head&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;body&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;video-grid&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/body&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/html&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;So far we have a server that generates a random room (with a random ID) everytime localhost:3000 is called&lt;/strong&gt;. The room page contains a blank div which will hold the video.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Add joining room function to server.js&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;io.on('connection', socket =&amp;gt; {
    socket.on('join-room', (roomId, userId) =&amp;gt; {
        console.log(roomId, userId)
    })
})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;roomId&lt;/strong&gt; is set as a constant in the JavaScript so is sent to the page&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;script&amp;gt;
    const ROOM_ID = &quot;&amp;lt;%= roomId %&amp;gt;&quot;
&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/gam750/webrtc-roomid.jpg&quot; alt=&quot;roomId sent to the page&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bring in the socket.io jasvascript code into our front end, served by our own server (server.js)&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;script src=&quot;/socket.io/socket.io.js&quot; defer&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Add a script file for our own JavaScript&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;script src=&quot;script.js&quot; defer&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Join room and broadcast userId to other clients&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;.. in &lt;strong&gt;script.js&lt;/strong&gt; (userId is hard-coded for now)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;const socket = io('/')

socket.emit('join-room', ROOM_ID, 10)

socket.on('user-connected', userId =&amp;gt; {
    console.log('User connected: ' + userId) })
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;.. in &lt;strong&gt;server.js&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;io.on('connection', socket =&amp;gt; {
    socket.on('join-room', (roomId, userId) =&amp;gt; {
        socket.join(roomId)
        socket.to(roomId).broadcast.emit('user-connected', userId)
    })
})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;When the second client joins the same room (same roomId) the first client is informed&lt;/strong&gt;
&lt;img src=&quot;/images/gam750/webrtc-join-room-1.jpg&quot; alt=&quot;joining the room&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;peerjs&quot;&gt;PeerJS&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;npm i -g peer&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://peerjs.com/&quot;&gt;The PeerJS library&lt;/a&gt;&lt;/strong&gt; allows us to run a &lt;strong&gt;peer server&lt;/strong&gt;, creating connections between different users using WebRTC.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Note - I’m hoping this is not going to prove to be a difficult part of the process of setting up WebRTC for Unity.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;peerjs –port 3001&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The peer server is now runing on poer 3001, which will allow us to connect users and will give us an ID to replace the hard-coded one above.&lt;/p&gt;

&lt;p&gt;Add the following (from https://peerjs.com/) to &lt;strong&gt;room.ejs&lt;/strong&gt; (using defer to ensure running first)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;script defer src=&quot;https://unpkg.com/peerjs@1.3.1/dist/peerjs.min.js&quot;&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Place the emit function (in script.js) into a function to run when connected to PeerJS&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This will provde the unique ID for the user that is connected.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;myPeer.on('open', id =&amp;gt; {
    socket.emit('join-room', ROOM_ID, id)
})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Get a reference to the video grid&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;const videoGrid = document.getElementById('video-grid')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;And a reference to a video&lt;/strong&gt; and mute it so we don’t hear our own voice played back to us.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;const myVideo = document.createElement('video')
myVideo.muted = true;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Connect our video&lt;/strong&gt;, returns a stream (as a promise), which will be listened to by an event listener (see function addVideoStream below)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;navigator.mediaDevices.getUserMedia({
    video: true,
    audio: true
}).then(stream =&amp;gt; {
    addVideoStream(myVideo, stream)
})

Function addVideoStream(video, stream){
    video.srcObject = stream
    video.addEventListener('loadedmetadata', () =&amp;gt; {
        video.play()
    })
    videoGrid.append(video)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;On refreshing our page, we can see our video appear in our page&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Permissions will be required the first time this is run.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Allowing ourselves to be connected to by other users&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Send our current video and audio stream to the new user&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;socket.on('user-connected', userId =&amp;gt; {
    connectToNewUser(userId, stream)
})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When they call back we take in their video stream - called ‘stream’ - [ call.on(‘stream’,… ] and add to our list of videos [ addVideoStream(video, userVideoStream) ]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;On close, remove the video from the page.&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function connectToNewUser(userId, stream) {
    const call = myPeer.call(userId, stream)
    const video = document.createElement('video')
    call.on('stream', userVideoStream =&amp;gt; {
        addVideoStream(video, userVideoStream)
    })
    call.on('close', () =&amp;gt; {
        video.remove()
    })
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We must listen for calls so we know if someone is calling us and then we can add them to our videos on screen.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;myPeer.on('call', call =&amp;gt; {
    call.answer(stream)
})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But we need to respond&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    const video = document.createElement('video')
    call.on('stream', userVideoStream =&amp;gt; {
        addVideoStream(video, userVideoStream)
    })
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So now we can receive calls by listening to our &lt;strong&gt;on call event&lt;/strong&gt; and make calls when new users connect to our room (connectToNewUser)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Finally we need to handle closing the videos better when a user disconnects&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Back in &lt;strong&gt;server.js&lt;/strong&gt;, on disconnect run another function (broadcast.emit) to send the event down to our room.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;socket.on('disconnect'), () =&amp;gt; {
    socket.to(roomId).broadcast.emit('user-disconnected', userId)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;then to pick up this event in script.js&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;socket.on('user-disconnected', userId =&amp;gt; {
    console.log(userId)
})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We need to use the userId to disconnect the user&lt;/p&gt;

&lt;p&gt;We can log the callers in an object&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;const peers = {}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and on connecting to new user add them to the object so they can be removed when disconnected&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;peers[userId] = call

socket.on('user-disconnected', userId =&amp;gt; {
    if(peers[userId]) peers[userId].close()
})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\webrtc-javascript-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;all-the-scripts-in-their-entirety&quot;&gt;All the scripts in their entirety&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;server.js&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;const express = require ('express')
const app = express()
const server = require ('http').Server(app)
const io = require('socket.io')(server)
const { v4: uuidV4 } = require('uuid')

app.set('view engine', 'ejs')
app.use(express.static('public'))

app.get('/', (req, res) =&amp;gt; {
    res.redirect(`/${uuidV4()}`)
})

app.get('/:room', (req, res) =&amp;gt; {
    res.render('room', { roomId: req.params.room })
})

io.on('connection', socket =&amp;gt; {
    socket.on('join-room', (roomId, userId) =&amp;gt; {
        socket.join(roomId)
        socket.to(roomId).broadcast.emit('user-connected', userId)

        socket.on('disconnect', () =&amp;gt; {
            socket.to(roomId).broadcast.emit('user-disconnected', userId)
        })
    })
})

server.listen(3000)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;room.ejs&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;&amp;lt;!DOCTYPE html&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;html&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;lang=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;head&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;meta&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;charset=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;UTF-8&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;meta&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;viewport&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;content=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;width=device-width, initial-scale=1.0&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;script&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;ROOM_ID&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;lt;%= roomId %&amp;gt;&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;script &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;defer&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://unpkg.com/peerjs@1.3.1/dist/peerjs.min.js&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;script &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/socket.io/socket.io.js&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;defer&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;script &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;script.js&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;defer&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;title&amp;gt;&lt;/span&gt;Document&lt;span class=&quot;nt&quot;&gt;&amp;lt;/title&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;style&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nf&quot;&gt;#video-grid&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;nl&quot;&gt;display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;py&quot;&gt;grid-template-columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;repeat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;auto-fill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;300px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
            &lt;span class=&quot;py&quot;&gt;grid-template-rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;300px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;video&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;nl&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;100%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;nl&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;100%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;nl&quot;&gt;object-fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cover&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;/style&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/head&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;body&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;id=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;video-grid&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/body&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/html&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;script.js&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;const socket = io('/')
const videoGrid = document.getElementById('video-grid')
const myPeer = new Peer(undefined, {
    host: '/',
    port: '3001'
})
const myVideo = document.createElement('video')
myVideo.muted = true;
const peers = {}

navigator.mediaDevices.getUserMedia({
    video: true,
    audio: true
}).then(stream =&amp;gt; {
    addVideoStream(myVideo, stream)

    myPeer.on('call', call =&amp;gt; {
        call.answer(stream)
        const video = document.createElement('video')
        call.on('stream', userVideoStream =&amp;gt; {
            addVideoStream(video, userVideoStream)
        })
    })

    socket.on('user-connected', userId =&amp;gt; {
        connectToNewUser(userId, stream)
    })
})

socket.on('user-disconnected', userId =&amp;gt; {
    if(peers[userId]) peers[userId].close()
})

myPeer.on('open', id =&amp;gt; {
    socket.emit('join-room', ROOM_ID, id)
})

function connectToNewUser(userId, stream) {
    const call = myPeer.call(userId, stream)
    const video = document.createElement('video')
    call.on('stream', userVideoStream =&amp;gt; {
        addVideoStream(video, userVideoStream)
    })
    call.on('close', () =&amp;gt; {
        video.remove()
    })

    peers[userId] = call
}

function addVideoStream(video, stream){
    video.srcObject = stream
    video.addEventListener('loadedmetadata', () =&amp;gt; {
        video.play()
    })
    videoGrid.append(video)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Run with..&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;npm run devStart
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;which runs the script in package.json&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;scripts&quot;: {
  &quot;devStart&quot;: &quot;nodemon server.js&quot;
},
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This has been an interesting and valuable lesson in peer-to-peer connectivity. I am still not sure I have much of a handle on WebRTC specifically as, unexpectedly, the project uses &lt;strong&gt;&lt;a href=&quot;https://peerjs.com/&quot;&gt;Peer JS&lt;/a&gt;&lt;/strong&gt;, an external JavaScript library to provide an API to handle WebRTC.&lt;/p&gt;

&lt;p&gt;However, I am probably - hopefully - in a better position to attempt implementing WebRTC in a Unity project.&lt;/p&gt;</content><author><name></name></author><summary type="html">As stated in the previous post, at the moment I feel that my time would be best spent learning about WebRTC in general, rather than specifically as a Unity plugin. Once I feel I fully understand the basics I should be able to apply them more easily to my specific Unity test-case.</summary></entry><entry><title type="html">Streaming To And From Unity</title><link href="http://localhost:4000/2020/07/25/streaming-into-unity.html" rel="alternate" type="text/html" title="Streaming To And From Unity" /><published>2020-07-25T15:00:00+01:00</published><updated>2020-07-25T15:00:00+01:00</updated><id>http://localhost:4000/2020/07/25/streaming-into-unity</id><content type="html" xml:base="http://localhost:4000/2020/07/25/streaming-into-unity.html">&lt;h1 id=&quot;the-requirement&quot;&gt;The requirement&lt;/h1&gt;

&lt;p&gt;I need to find a way to take data from a device and stream it into my Unity scene.&lt;/p&gt;

&lt;p&gt;An example would be taking video from a webcam or a mobile phone camera, creating a data stream to which a Unity script can subscribe and then display the streamed video onto a texture within the scene.&lt;/p&gt;

&lt;p&gt;Another, perhaps simpler example, would be to take numeric 2D positional data, manipulated on the screen of the extertanl device, and stream that to Unity.  The data can then be used to control the position of an object within the scene.&lt;/p&gt;

&lt;p&gt;Audio would be another, allowing a stream of audio data from the external device’s microphone to enter the Unity scene and be heard by the players via the audio mixer.&lt;/p&gt;

&lt;h1 id=&quot;thinking-about-options&quot;&gt;Thinking about options&lt;/h1&gt;

&lt;p&gt;Would it be best, or feasible, to create a room in the Unity project that can access the device’s camera resources, and then to create a build for each platform - i.e. Android (can be the same build as for the Oculus quest), iOs and Windows.&lt;/p&gt;

&lt;p&gt;Or could the unity project be cloud-hosted, allowing users to connect via the internet. The cloud app would need to be able to access the device’s resources across the internet, much like Zoom or Skype etc.&lt;/p&gt;

&lt;p&gt;An alternative may be to create a web app that can run in a browser - perhaps raw JavaScript or an Angular/Vue/React etc. app, which can access the device’s resources and stream via a socket to a server. The Unity project can then connect to a socket on the server and subscribe to the datastream.&lt;/p&gt;

&lt;p&gt;Other than these options we could be looking at either a cross-platform option, like Flutter or Ionic, or multiple Native apps.&lt;/p&gt;

&lt;h1 id=&quot;random-research&quot;&gt;Random research&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://stackoverflow.com/questions/37100900/unity-processing-data-stream-from-socket&quot;&gt;StackOverflow&lt;/a&gt;&lt;/strong&gt;: Unity Processing Data Stream from Socket.  This is an interesting question asked on StackOverflow and it leads to a &lt;strong&gt;potential solution&lt;/strong&gt; for setting up a TCP client in Unity: &lt;strong&gt;&lt;a href=&quot;https://stackoverflow.com/questions/36526332/simple-socket-server-in-unity/36526634#36526634&quot;&gt;Simple socket server in Unity&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://stackoverflow.com/questions/46564222/how-to-send-and-receive-tcp-messages-while-streaming-video-unity-and-socket-ne&quot;&gt;Another on StackOverflow&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://stackoverflow.com/questions/42717713/unity-live-video-streaming/42727918#42727918&quot;&gt;And another…&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://stackoverflow.com/questions/17719541/writing-and-reading-using-socket&quot;&gt;… and guess what…&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mozilla:&lt;/strong&gt; &lt;strong&gt;&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API&quot;&gt;The WebSocket API (WebSockets)&lt;/a&gt;&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API/Writing_WebSocket_server&quot;&gt;Writing a WebSocket server in C#&lt;/a&gt;&lt;/strong&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another possible option could be to re-use the code already implemented in my &lt;strong&gt;&lt;a href=&quot;/2020/07/11/client-server-tutorial.html&quot;&gt;C# TCP/UDP multiplayer experiment&lt;/a&gt;&lt;/strong&gt;.  As it is basically about sending and receiving data between clients via a server, in a sense it is already all in place. I’m tempted to combine this with what I could learn from the above Mozilla WebSocket documentation and to build the solution from the ground up.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://webrtc.org/&quot;&gt;WebRTC&lt;/a&gt;&lt;/strong&gt;: I keep coming back to this - it’s potentially the all-encompassing solution, covering browser, native, Unity - both client and server, both directions possible (or so it would seem).
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;/2020/07/02/webrtc.html&quot;&gt;My earlier post on WebRTC&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://microsoft.github.io/MixedReality-WebRTC/manual/gettingstarted.html&quot;&gt;Microsoft’s WebRTC project&lt;/a&gt;&lt;/strong&gt;, including a library for &lt;strong&gt;&lt;a href=&quot;https://microsoft.github.io/MixedReality-WebRTC/manual/unity/unity-integration.html&quot;&gt;Unity&lt;/a&gt;&lt;/strong&gt; and a &lt;strong&gt;&lt;a href=&quot;https://microsoft.github.io/MixedReality-WebRTC/manual/unity/helloworld-unity.html&quot;&gt;Unity tutorial&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://docs.unity3d.com/Packages/com.unity.webrtc@2.0/manual/index.html&quot;&gt;Unity documentation&lt;/a&gt;&lt;/strong&gt;: includes links to a &lt;strong&gt;&lt;em&gt;Tutorial&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;Reference&lt;/em&gt;&lt;/strong&gt; for &lt;strong&gt;&lt;em&gt;Video&lt;/em&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;em&gt;Audio&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;Data&lt;/em&gt;&lt;/strong&gt; streaming.
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/Unity-Technologies/UnityRenderStreaming/blob/release/1.0.0/Packages/com.unity.template.renderstreaming/Documentation~/en/tutorial.md&quot;&gt;Unity Render Streaming&lt;/a&gt;&lt;/strong&gt;: streaming video out from Unity project using &lt;strong&gt;WebRTC&lt;/strong&gt; - I’m not sure just yet how useful this could be - whether it is just from the editor or from a live build. Either way, it looks interesting.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;unity-assets&quot;&gt;Unity Assets&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://assetstore.unity.com/packages/templates/packs/fmetp-stream-143080#reviews&quot;&gt;FMETP&lt;/a&gt;&lt;/strong&gt; This asset may do the trick on a number of scores - video in and out.&lt;br /&gt;
&lt;strong&gt;&lt;a href=&quot;https://forum.unity.com/threads/release-fmetp-stream-all-in-one-gameview-audio-stream-udp-tcp-websockets-html.670270/&quot;&gt;FMETP - Unity forum&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;This certainly does look like an interesting solution. The devlelopers are very much on the ball, judging by the various queries and reqests, so even if if it not directly perfect for my use, it should be possible to work with the developers in finding a solution. 
However, even as it is, it could provide a solution to my requiments - &lt;a href=&quot;https://answers.unity.com/questions/1509753/stream-live-video-feed-into-unity.html&quot;&gt;see here&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://innovativeteams.net/using-socketio-build-multi-user-experiences/&quot;&gt;Using Socket.io and Unity 3D To Build Multi-User Experiences&lt;/a&gt;&lt;/strong&gt; This makes use of the Socket.IO for Unity (see above).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://tokbox.com/blog/add-opentok-live-video-chat-to-unity/&quot;&gt;OpenTok (2017 post)&lt;/a&gt;&lt;/strong&gt;&lt;br /&gt;
&lt;strong&gt;&lt;a href=&quot;https://tokbox.com/blog/unity-opentok-take-two/?utm_source=rss&amp;amp;utm_medium=rss&amp;amp;utm_campaign=unity-opentok-take-two&quot;&gt;OpenTok (2019 post)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://assetstore.unity.com/packages/tools/video/agora-video-chat-sdk-for-unity-134502&quot;&gt;Agora&lt;/a&gt;&lt;/strong&gt;
Unity asset - another out of the box solution. It is &lt;a href=&quot;https://www.agora.io/en/pricing/&quot;&gt;well priced&lt;/a&gt; and could be very useful. I’d rather build my own though…&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://www.because-why-not.com/webrtc/&quot;&gt;WebRTC Video Chat asset by Because Why Not?&lt;/a&gt;&lt;/strong&gt;: This costs (about £100) for the asset.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;other&quot;&gt;Other&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://www.liveswitch.io/&quot;&gt;liveswitch cloud&lt;/a&gt;&lt;/strong&gt;: has monthly pricing - $19 plus per minute payment&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://www.nuget.org/packages/Spitfirex86/&quot;&gt;Spitfirex86&lt;/a&gt;&lt;/strong&gt;: NuGet package - for .NET applications rather than specifically Unity.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2019/12/amazon-kinesis-video-streams-adds-support-for-real-time-two-way-media-streaming-with-webrtc/&quot;&gt;Amazon Kinesis Video Streams&lt;/a&gt;&lt;/strong&gt;: &lt;em&gt;“Kinesis Video Streams includes a WebRTC signaling end-point for fast peer discovery and secure connection establishment. It includes managed Session Traversal Utilities for NAT (STUN) and Traversal Using Relays around NAT (TURN) end-points for real-time exchange of media between peers.”&lt;/em&gt; You pay for what you use - looks reasonably priced.
&lt;strong&gt;&lt;a href=&quot;https://docs.aws.amazon.com/kinesisvideostreams-webrtc-dg/latest/devguide/kvswebrtc-how-it-works.html&quot;&gt;How it works&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://medium.com/@omidborjian/setup-your-own-turn-stun-signal-relay-server-on-aws-ec2-78a8bfcb71c3&quot;&gt;Set up TRUN/STUN signal/relay server on AWS EC2&lt;/a&gt;&lt;/strong&gt;: could be very useful&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/webrtc/FirebaseRTC&quot;&gt;Firebase + WebRTC&lt;/a&gt;&lt;/strong&gt;
&lt;strong&gt;&lt;a href=&quot;https://webrtc.org/getting-started/firebase-rtc-codelab&quot;&gt;Getting started&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://webrtc.github.io/samples/&quot;&gt;WebRTC Samples&lt;/a&gt;&lt;/strong&gt;: a collection of small samples demonstrating various parts of the WebRTC APIs&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;moving-forward&quot;&gt;Moving forward…&lt;/h1&gt;

&lt;p&gt;I have been trying to find a simple, straightforward, way of introducing WebRTC into a Unity project. However, I have had trouble whichever way I’ve turned. The Microsoft package looks interesting, but my implementation via &lt;strong&gt;NuGet&lt;/strong&gt; did not introduce the Unity sample scripts and components as described. The full package implementation (i.e. before even considering the implementation of the WebRTC scripts etc. and getting a working solution up and running) could, I suppose, be studied in detail, but it looks pretty complex and I’m not sure that spending too much of my time doing so would be time well spent as there is no certainty that the result would be successful.&lt;/p&gt;

&lt;p&gt;The Unity documentation itself is not very complete, in fact I’d characterise it as being sparce. It seems to explain snippets of code as though you already have them set up in your project, without explaining how to actually set them up in your project or give a wider context to them. It appears to be another case (I come across this fairly often) of documentation written by developers who assume you have been on the journey with them, but have been easily distracted.&lt;/p&gt;

&lt;p&gt;I also tried following the Unity Render Streaming tutorial but, less than a year on, it is already out of date. I had trouble with the latest releases being quite different in terms of file content and the resulting WebRTC templates for Unity were not in line with what was being demonstrated in the video. I’m sure there are more tutorials out there, but for now &lt;strong&gt;I feel that my time would be better spent actually learning about WebRTC in general, rather than as a Unity plugin&lt;/strong&gt;. Once I have the basics, and really understand them, I should be able to apply them more easily to my specific Unity test-case.&lt;/p&gt;</content><author><name></name></author><summary type="html">The requirement</summary></entry></feed>