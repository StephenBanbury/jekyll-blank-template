<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-09-14T23:19:26+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Creative Reflective Journal</title><subtitle>My Creative Reflective Journal towards the Falmouth University Creative App Development Masters Degree.</subtitle><entry><title type="html">Sending and Receiveing OSC Data</title><link href="http://localhost:4000/2020/09/14/sending-data.html" rel="alternate" type="text/html" title="Sending and Receiveing OSC Data" /><published>2020-09-14T01:00:00+01:00</published><updated>2020-09-14T01:00:00+01:00</updated><id>http://localhost:4000/2020/09/14/sending-data</id><content type="html" xml:base="http://localhost:4000/2020/09/14/sending-data.html">&lt;p&gt;I’ve been thinking about how to send data between clients. The requirement is to make use of one of the two platforms I have put in place to keep clients connected - i.e. &lt;strong&gt;Normcore&lt;/strong&gt; for multiplayer functionality and &lt;strong&gt;Agora&lt;/strong&gt; for live video and audio streaming - in order to send simple data that can be used and interpreted in a number of ways, with a number of possible functionalities in mind.&lt;/p&gt;

&lt;h1 id=&quot;osc-addressing&quot;&gt;OSC addressing&lt;/h1&gt;
&lt;p&gt;The main one here is &lt;strong&gt;OSC&lt;/strong&gt; messages, used to control screen content, formations, lighting, sound, and many other potential uses that could come up as this project continues to be developed. As this data is simply text, it could be passed in form of JSON or raw strings, and then easily interpreted by clients connected in.&lt;/p&gt;

&lt;p&gt;But there could potentially be other uses for such data channels. Perhaps they could be used to send &lt;em&gt;coordinate&lt;/em&gt; data for the screen formations and, more interestingly, the &lt;em&gt;players within the VR space&lt;/em&gt;. These could then be used in mappying out the space in &lt;em&gt;real life&lt;/em&gt; in order for the performers to keep tabs on where players are and on their movements.&lt;/p&gt;

&lt;h1 id=&quot;agora&quot;&gt;Agora&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Agora&lt;/strong&gt; provides a simple data channel that can be used for sending text messages from broadcaster to clients. I am currently thinking that this could be very useful for sending data in JSON format.&lt;/p&gt;

&lt;p&gt;I have tested it with simple &lt;em&gt;‘hello’&lt;/em&gt; messages.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Message sent from Video Streaming client and received by VR client&lt;/strong&gt;
&lt;img src=&quot;\images\GAM750\agora-data-stream-message-1.JPG&quot; alt=&quot;message 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Message sent from VR client and received by Video Streaming client&lt;/strong&gt;
&lt;img src=&quot;\images\GAM750\agora-data-stream-message-2.JPG&quot; alt=&quot;message 2&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;normcore&quot;&gt;Normcore&lt;/h1&gt;
&lt;p&gt;I am very tempted to hook up both the video streaming and the OSC controller client apps to Normcore and allow data to be pushed around using Normcore data-pooling techniques. This could work in exactly the same as does sending video, screen and screen formation selection currently does between clients that have joined the room. The advantage of this method could be that it would be a relatively simple task to share &lt;strong&gt;all&lt;/strong&gt; data that is being used by &lt;strong&gt;all&lt;/strong&gt; of the client apps involved. It would not require specific extra channels, methodologies and means of sending and receiving data. The way the data is packaged and then interpreted at the receiving end will be the same, but pooling the data via Normcore will allow it to be accessible by any client at any time.&lt;/p&gt;

&lt;h1 id=&quot;decision&quot;&gt;Decision&lt;/h1&gt;
&lt;p&gt;The simplicilty of sending text messages using Agora is tempting me in this direction. There is little to set up and it will not require writing methods to keep the data in sync between clients.&lt;/p&gt;

&lt;p&gt;I feel the way forward will be to include both Normcore and Agora in all three apps, so that any can be called upon where necessary. But for the time being, I will explore the use of Agora’s simple messaging ability to for OSC addressing.&lt;/p&gt;

&lt;p&gt;When it comes to finding ways to keep performers (i.e. video streaming clients) informed as to the positioning of audience, it may be that I will turn to Normcore.&lt;/p&gt;</content><author><name></name></author><summary type="html">I’ve been thinking about how to send data between clients. The requirement is to make use of one of the two platforms I have put in place to keep clients connected - i.e. Normcore for multiplayer functionality and Agora for live video and audio streaming - in order to send simple data that can be used and interpreted in a number of ways, with a number of possible functionalities in mind.</summary></entry><entry><title type="html">The Possibilities Of OSC</title><link href="http://localhost:4000/2020/09/13/osc-possibilities.html" rel="alternate" type="text/html" title="The Possibilities Of OSC" /><published>2020-09-13T01:00:00+01:00</published><updated>2020-09-13T01:00:00+01:00</updated><id>http://localhost:4000/2020/09/13/osc-possibilities</id><content type="html" xml:base="http://localhost:4000/2020/09/13/osc-possibilities.html">&lt;h1 id=&quot;osc-controller&quot;&gt;OSC Controller&lt;/h1&gt;

&lt;p&gt;I created an Unity project for the purpose of developing an &lt;strong&gt;OSC controller&lt;/strong&gt;. After importing the &lt;em&gt;OSC Simpl&lt;/em&gt; Unity package, I studied the example apps and the documentation - which is concise but not rich enough to rely on without a fair bit of code-analysis - and managed to cobble together an OCS controller with buttons that replace the temporary VR interface I have been using to control the environment from within.&lt;/p&gt;

&lt;p&gt;The main app also contains the OSC package and listens for OSC data messages. Once received, the message fires exactly the same actions as the temporary interface. Therefore, the desired effects not only occur, but they sync across &lt;em&gt;Normcore&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Currently, the OSC messages are in the form: -&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;/select/video&lt;/li&gt;
  &lt;li&gt;/select/stream&lt;/li&gt;
  &lt;li&gt;/select/display&lt;/li&gt;
  &lt;li&gt;/select/formation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The /select/ part indicates that a selection has been made.
/video/, /stream/ etc. indicates what kind of action we are going to request.&lt;/p&gt;

&lt;p&gt;Following this, an argument is passed indicating the &lt;strong&gt;ID&lt;/strong&gt; of the video, the video stream, the display screen, or the screen formation…&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    const string _videoSelectAddress = &quot;/select/video&quot;;
    const string _streamSelectAddress = &quot;/select/stream&quot;;
    const string _displaySelectAddress = &quot;/select/display&quot;;
    const string _formationSelectAddress = &quot;/select/formation&quot;;

    public void VideoSelect(int videoId)
    {
        oscOut.Send(_videoSelectAddress, videoId);
	    }
    public void StreamSelect(int streamId)
    {
        oscOut.Send(_streamSelectAddress, streamId);
    }
	public void DisplaySelect(int displayId)
    {
        oscOut.Send(_displaySelectAddress, displayId);
	    }
    public void FormationSelect(int formationId)
    {
        oscOut.Send(_formationSelectAddress, formationId);
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;… and at the listening end, these patterns are looked for and responces are wired up. Therefore a specific message plus arguments is sent, and this results in a specific method being called at the receiver’s end.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\images\GAM750\osc-interface-1.JPG&quot; alt=&quot;OSC Interface&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\osc-unity-demo-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\osc-demo-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;should-be-fun&quot;&gt;Should be fun&lt;/h1&gt;

&lt;p&gt;Of course, this follows the pattern already in place in the main VR app, i.e. when selecting the video or stream to display, the ID is selected first, followed by the ID of the display on which the respective media is to be shown.&lt;/p&gt;

&lt;p&gt;However, as suggested by my fellow student and resident OSC expert, Sam Smallman, on our Discord server forum, there are other possible ways to provide this information: -&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Sam:&lt;/strong&gt; You can use wildcards to help so you would basically receive an OSC message and pattern match for “/fireclown/screen/*/hidden” then you would grab the 3rd part which would be the screen number and then take the argument value and hide unhide the screen from there.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Have you considered stateless commands, so /formation 3 “1 2 3 4 5”, basically set formation to 3(1st argument) to the screens 1 thru 5 (2nd string argument)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;then you could have /screen/1/formation 3, to set indivual screen 1 to formation 3
.. ah no because formation is a global thing but video feed, /screen/1/video 3, to set indivdual screen 1 to video feed 3. There’s a lot of fun you can have with this&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Me:&lt;/strong&gt; My big issue, though, is how to set it up to send over the internet. Probably have to send it via Agora - which I’m using for the video streaming but also has a data channel - or normcore somehow… I think you’ve mentioned before that theatre is usually done in a closed network.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Sam:&lt;/strong&gt; it might be easier to just send http post requests with a json packet as the arguments, and then you can translate them to osc&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;oh yeah, most theatre wont be online ever. Things are changing at the moment but still there’s massive superstition about putting anything online during a performance.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Me:&lt;/strong&gt; As this project is all about remote working, I have no choice in the matter.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Sam:&lt;/strong&gt; you would need to wrap osc anyway to get over the net you may aswell just wrap it in http&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;as a theatre practitioner my next questions would be what control over the screens do i have. Does screen feeds snap from one to another, do i have control of fading it,  do i have control of the formations and how they move from one to another and the time they take, can i indivdually control screen brightness etc. All of that I would want control via OSC arguments etc.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So I’m going to have fun with OSC once we’ve nailed down the requirements and in what contexts we think it could be most useful. I think the main difficulty will prove to be finding the best (and, as always, the least time-consuming) way to get the data across the internet. I feel this may best be achieved by including Agora or Normcore in the OSC project and sending the data wrapped in JSON to the main VR app. Once it has arrived, it can then be interpretted and acted upon.&lt;/p&gt;

&lt;p&gt;In a way, this will negate the need for the OSC Simple package, as there will likely be no need to broadcast via http and listen at the other end. However, it could be more flexible in the long run to use as much of the basic OSC broadcasting pattern and the technology behind it as possible, and as little as possible of the mechanism used to send the data occross the &lt;em&gt;World Wide Web&lt;/em&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">OSC Controller</summary></entry><entry><title type="html">Creating The Screens 2</title><link href="http://localhost:4000/2020/09/03/creating-the-screens-2.html" rel="alternate" type="text/html" title="Creating The Screens 2" /><published>2020-09-03T01:00:00+01:00</published><updated>2020-09-03T01:00:00+01:00</updated><id>http://localhost:4000/2020/09/03/creating-the-screens-2</id><content type="html" xml:base="http://localhost:4000/2020/09/03/creating-the-screens-2.html">&lt;h1 id=&quot;animations&quot;&gt;Animations&lt;/h1&gt;

&lt;p&gt;I came across an interesting-looking Unity animation asset - &lt;strong&gt;&lt;a href=&quot;http://dotween.demigiant.com/&quot;&gt;DOTween&lt;/a&gt;&lt;/strong&gt; - whilst working on the ShelterBox project. In fact, so interesting it looked I purchased the &lt;strong&gt;&lt;a href=&quot;http://dotween.demigiant.com/pro.php&quot;&gt;‘pro’ version &lt;/a&gt;&lt;/strong&gt; before even using it; I had a feeling it would come in handy in future projects even if I didn’t get to use it at the time. And, sure enough, this has been the case.&lt;/p&gt;

&lt;p&gt;DOTween has many features, but the main attration for me was the way it allows complex tweening and manipulation of animations via code and, as such, can be controlled within the codebase of the project without having to resort to the somewhat ‘clunky’ (in my opinion) animation creation tools available out of Unity the box.&lt;/p&gt;

&lt;p&gt;I have used it here in order to tween between one set of &lt;em&gt;Vector3&lt;/em&gt; and &lt;em&gt;Rotation&lt;/em&gt; data for a formation of screens and another. So it turns out my method of recording the coordinates data for each formation, as outlined in my previous post, was &lt;strong&gt;the correct thing to do!&lt;/strong&gt;&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\screenanimations-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\screenanimations-2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;creating-the-formations&quot;&gt;Creating the formations&lt;/h1&gt;

&lt;p&gt;Creating the actual formations involved much laborious moving and positioning of screens in the Unity editor, as well as some &lt;strong&gt;calculating&lt;/strong&gt; of coordinates in order to create specific shapes. For example, the &lt;strong&gt;isosceles triangle&lt;/strong&gt; required the three angles to be calculated depending on the lengths of the sides. As there are 16 screens, the triangle ended up having two sides comprising six screens and one side of four. To save me time, I used an &lt;strong&gt;&lt;a href=&quot;https://www.triangle-calculator.com/?what=iso&quot;&gt;online calculation tool&lt;/a&gt;&lt;/strong&gt;. The Angles of the points of the triangle were therefore calculated thus:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;\images\GAM750\calc-isoscelese-triangle.JPG&quot; alt=&quot;Isoscelese triangle calculation&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">Animations</summary></entry><entry><title type="html">Creating The Screens 1</title><link href="http://localhost:4000/2020/09/01/creating-the-screens-1.html" rel="alternate" type="text/html" title="Creating The Screens 1" /><published>2020-09-01T01:00:00+01:00</published><updated>2020-09-01T01:00:00+01:00</updated><id>http://localhost:4000/2020/09/01/creating-the-screens-1</id><content type="html" xml:base="http://localhost:4000/2020/09/01/creating-the-screens-1.html">&lt;h1 id=&quot;the-screen-object&quot;&gt;The screen object&lt;/h1&gt;

&lt;p&gt;Initial prototypes were based on simplicity and guess-work. In fact they were too small and not quite the correct dimension. They also had no detail but were, instead, simple slabs.&lt;/p&gt;

&lt;p&gt;Dimensions were later provided for the screen alone and for its frame, along with resourses including model diagrams, sketches and a SketchUp model. The latter I have not been able to import as it is created within a newer version of SketchUp than mine - but I will see if there is a way.&lt;/p&gt;

&lt;p&gt;There is a very specific visual identity for the screen, which the director is keen to use. The physical screens have frames which are connected and hinged, allowing them to be folded and maneouvred, in order to create spaces that can morph into differ in shapes and sizes.&lt;/p&gt;

&lt;p&gt;The screen material itself is designed for projections. However, in my case, I needed to include a video player for local video material and a canvas onto which an Agora Surface can be created. Both of these are attached to the containing screen object as children and can be switched on and off as required.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Single screen&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\single-screen-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Multiple screens&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\multiple-screens-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;screen-formations&quot;&gt;Screen Formations&lt;/h1&gt;

&lt;p&gt;Formations need to be able to morph, from one formation to another. I have been considering how this should best be done: -&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Unity physics&lt;/strong&gt;: in this scenario each frame would be hinged and when one is physically moved by a virtual presense in the space or other means, then the screens attached will also be affected. This seems a nice-to-have feature, but could be fraught with difficulties, particularly in the short space of time available.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Plotting the locations&lt;/strong&gt; of the screens on the ground, and then moving the points over time, dragging the screens into new positions, creating new formations. This would appear to provide a lot of control over where and how the screens move, but could be time-consuming.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Unity animations&lt;/strong&gt;: this could a relatively straighforward option. Would this be for individual screens - all 16 of them - or for a formation as a whole? This would be a major question when formulating a solution.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Other animation methods&lt;/strong&gt;: using DoTween or similar could be an interesting way forward. However, I have not used the asset yet, although I did pay for the Pro version some months back. DoTween looks a good package, but I’d need to study and experiment with it first.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;spawning-screens&quot;&gt;Spawning screens&lt;/h1&gt;

&lt;p&gt;I created a &lt;strong&gt;class model&lt;/strong&gt; for screen position that holds the &lt;em&gt;Vector3&lt;/em&gt; coordinates and rotation data, in degrees, and a &lt;strong&gt;service&lt;/strong&gt; to provide lists of position data for each screen in a formation.&lt;/p&gt;

&lt;p&gt;This data is then read and assigned to each screen as it is instantiated into the space. Calling the method easily allows each formation to be immediately created. Should I end up using such data to move between two points (e.g. x1 =&amp;gt; x2, y1 =&amp;gt; y2, z1 =&amp;gt; z2) then this will be very useful. Even if this is not the final method I use, it is very useful to hold this data, as a record, in order to recreate formations whenever I need to, whether this is during runtime or during development and editing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Simple formations&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\screen-formations-1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Example formation with associated coordinates&lt;/strong&gt;
&lt;img src=&quot;\images\GAM750\formation-1.jpg&quot; alt=&quot;Screen Formation Image&quot; /&gt;
&lt;img src=&quot;\images\GAM750\formation-1-coordinates.jpg&quot; alt=&quot;Screen Formation Image Coordinates&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">The screen object</summary></entry><entry><title type="html">Visuals</title><link href="http://localhost:4000/2020/08/31/visuals.html" rel="alternate" type="text/html" title="Visuals" /><published>2020-08-31T01:00:00+01:00</published><updated>2020-08-31T01:00:00+01:00</updated><id>http://localhost:4000/2020/08/31/visuals</id><content type="html" xml:base="http://localhost:4000/2020/08/31/visuals.html">&lt;h1 id=&quot;why&quot;&gt;Why?&lt;/h1&gt;

&lt;p&gt;At this stage, I felt it important to spend some time working on the visual aspects of the project. The reason for this was that, as much as anything else, I’m sure the &lt;em&gt;Director/Designer&lt;/em&gt; and &lt;em&gt;Technical Director&lt;/em&gt; would appreciate seeing something that is less prototype-like, less of a &lt;em&gt;MVP&lt;/em&gt; (minimal viable product), and with the capacity to provide a little more of a sense of potential, of what the space could look like and how it can be experienced. This would provide food for the imagination, which could only serve to feed the creative and collaborative process across the team as a whole.&lt;/p&gt;

&lt;p&gt;My graphic-creating, shaping, modeling and illustration skills are not something I’ve worked on to any great extent. I have dabbled, particularly with the ShelterBox project, with fairly simple shapes creating the environment, brought to life through animation, thoughful colouring and lighting. But I have not come close to mastering the visual arts of creating something visually unique, interesting stylistically or aesthetically. It’s fair to say I’ve done as much as I have been able to given time restraints and the more pressing need to create something that works well and looks a cut above &lt;em&gt;‘good enough’&lt;/em&gt;!&lt;/p&gt;

&lt;p&gt;Two areas that could be disgnated for such attention are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;The screens&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;The selector panels&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Why?</summary></entry><entry><title type="html">OSC (Open Sound Control)</title><link href="http://localhost:4000/2020/08/19/osc.html" rel="alternate" type="text/html" title="OSC (Open Sound Control)" /><published>2020-08-19T01:00:00+01:00</published><updated>2020-08-19T01:00:00+01:00</updated><id>http://localhost:4000/2020/08/19/osc</id><content type="html" xml:base="http://localhost:4000/2020/08/19/osc.html">&lt;h1 id=&quot;what-is-osc&quot;&gt;What is OSC?&lt;/h1&gt;

&lt;p&gt;A requirement of the &lt;strong&gt;&lt;em&gt;Not Near Enough&lt;/em&gt;&lt;/strong&gt; brief is to explore the possibility of &lt;em&gt;‘external control of the virtual theatre space’&lt;/em&gt;. &lt;strong&gt;QLab&lt;/strong&gt; is mentioned, as is &lt;strong&gt;OSC&lt;/strong&gt;. Neither of these meant anything to me at the start of this project. I have begun examining OSC, as it appears to be the protocol of choice for theatre equipment interaction.&lt;/p&gt;

&lt;p&gt;A series of blogs by &lt;strong&gt;Sam Smallman&lt;/strong&gt; - a fellow Creative App Developer on the Falmouth MA course - explains OSC: -&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.etcconnect.com/2017/08/exploring-the-network-osc-part-i/&quot;&gt;Exploring the Network: The Postcard (OSC)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.etcconnect.com/2017/11/exploring-the-network-the-address/&quot;&gt;Exploring the Network: The Address&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.etcconnect.com/2018/01/exploring-the-network-the-postman/&quot;&gt;Exploring the Network: The Postman&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.etcconnect.com/2019/02/exploring-the-network-osc-part-ii/&quot;&gt;Exploring the Network: OSC Connections&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the case of this project, OSC could be used for theatre technical crew to control, for example, the selection of video clips or available video streams, and their subsequent assignment to a particular selection of screens. It could also be used to control lighting, animations, movement of screens and so on. I also imagine it will be able to be used, potentially, in controlling sound, in terms of volume or routing.&lt;/p&gt;

&lt;p&gt;The main question appears to be, how will it work in a distributed, virtual theatre environment? 
Most theatres would be using a closed network and would not need to access the internet in order for equipment to talk to each other. In our case, the very nature of the virtual theatre space is distributed, by definition. All signals will need to be directed across the internet.&lt;/p&gt;

&lt;p&gt;It also follows that, if a technician is controlling the environment, they would need visual access to it, not least to see how it is being affected by their actions. How this can made to happen is a major question that will need to be thought through.&lt;/p&gt;

&lt;p&gt;My early thoughts on this are that any OSC-outputting equipment would need to connect to a local client, which would, in turn, be connected into the multiplayer space (i.e. via Normcore). Then, any changes detected by the local client would be treated in exactly the same way as changes currently are.  Normcore will facilitate changes to values being passed on to all other clients connected into the multiplayer space.&lt;/p&gt;

&lt;p&gt;This is getting a little more complicated by the day. I feel a diagram coming on!&lt;/p&gt;</content><author><name></name></author><summary type="html">What is OSC?</summary></entry><entry><title type="html">Using Agora</title><link href="http://localhost:4000/2020/08/18/using-agora.html" rel="alternate" type="text/html" title="Using Agora" /><published>2020-08-18T01:00:00+01:00</published><updated>2020-08-18T01:00:00+01:00</updated><id>http://localhost:4000/2020/08/18/using-agora</id><content type="html" xml:base="http://localhost:4000/2020/08/18/using-agora.html">&lt;h1 id=&quot;progress-update&quot;&gt;Progress update&lt;/h1&gt;
&lt;p&gt;I have not posted for more than a week now, and this is due to the fact that nearly all my dev time (there has been family time too, or I’ll end up with only dev time in my life - not a great prospect!) has been getting to know Agora and adapting the slightly haphazard demo material for use in my own VR app. There have been frustrating moments where things just do not work as you’d hope, but there has never really been a moment when I’ve thought “this is just never going to work”, or “if this is going to work, then it’s going to be more hassle than it’s worth!”. But I have continually been aware that it may be a laborious process, and this has been the case.&lt;/p&gt;

&lt;h1 id=&quot;what-just-streaming&quot;&gt;What?! Just streaming?!&lt;/h1&gt;

&lt;p&gt;I initially set up screens to which I allowed the attaching of Agora’s &lt;em&gt;VideoSurface&lt;/em&gt; component class. Once this was working, I needed to add back in the ability to diplay video clips. To this end I have a general ‘media’ class that covers both (and will cover any new) media types. Each screen object has attached, as a child, the already-existing screen object, that includes a VideoPlayer for video clips, and now, also, a canvas, to which an Agora VideoSurface can be dynamically attached as and when a stream is created and assigned to the display.&lt;/p&gt;

&lt;p&gt;To the Normcore multiplayer model I therefore needed to add another variable representing the selected video stream. I can now select either a pre-loaded video clip or a video stream provided by an external client device’s camera, and then assign it to a specified screen.  Depending on its type, the media will be displayed as a video clip by the video player or a video stream by the Agora video surface. If either a video or a stream is selected, followed by a display screen selection, the Normcore model triggers an event that informs all other clients to also select the same media and assign it to the same display screen.&lt;/p&gt;

&lt;p&gt;Buttons to select specific video streams provided by Agora clients are dynamically created as each client joins the room. As they join, they are added to the client list (of AgoraUsers). Clients are never removed, but if they leave the room they are marked as such by a &lt;em&gt;LeftRoom&lt;/em&gt; property.  Each is assigned an ID that is used to select the specific video stream.&lt;/p&gt;

&lt;p&gt;I am using a simple client app, based on the Agora demo, to provide a portal for external clients to join the room and create a video stream. This app uses the same Agora ID as the VR app (where streams are displayed) so streams are seen by the VR app, along with all other clients.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Streaming and playing local videos to selected screens&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\streaming-video-to-different-screens-2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;problems&quot;&gt;Problems&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Currently, when a new VR app instance is created, this is viewed as a new client by the other clients, including those that are other instances of the VR app. Therefore, an Agora stream is created for each instance and appears as a new client, including a new select button, in the VR environment. I need to find a way to treat each of these as ‘dead’ streams - they should not appear at all. The VR users should only receive streams to be displayed. This may be different when the app is build for Android - I have not tested this yet - but when run as a PC app and tested via &lt;strong&gt;Oculus Link&lt;/strong&gt;, the PC’s camera is added to the stream pool.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;This has been fixed, but I’ll keep the following for the record&lt;/strong&gt;: 
I’m seeing problems where one or more of the video feeds is freezing. This does not happen straight away, but after adding more and playing with how they are routed to different screen destination. I’m not sure if this is something that is to do with streaming/bandwidth performance, or similar, or something to do with the way the Unity Agora asset works. It could be that only one live stream is allowed at a time, although my observations in a previous post would suggest that this should not be a problem:&lt;/p&gt;

    &lt;blockquote&gt;
      &lt;p&gt;The Unity asset is for ‘live interactive video streaming’. This is geared towards one single main video channel at a time streamed to multiple locations. There is another option for ‘democratic’ open channels where all can communicate at the same time to all others.&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;Of course, it could be that I have misunderstood the documentation, and the difference between &lt;strong&gt;&lt;em&gt;&lt;a href=&quot;https://docs.agora.io/en/Voice/product_voice?platform=All%20Platforms&quot;&gt;Voice Call&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;&lt;a href=&quot;https://docs.agora.io/en/Interactive%20Broadcast/product_live?platform=All%20Platforms&quot;&gt;Live Interactive Video Streaming&lt;/a&gt;&lt;/em&gt;&lt;/strong&gt;. I will be examining these descriptions closely! This is something that I very much hope is not the case, because there is no Unity asset for Void Call!&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;.. the issue turned out to be nothing of the sort and has now been fixed. I needed to ensure that any existing Agora video surfaces would be re-used if any new streams were to be displayed on its screen, i.e. replacing the existing stream. Previously, I had been destroying these existing surfaces and then creating a new one.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Streaming and playing local videos in multiplayer mode&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\streaming-video-multiplayer-3.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;</content><author><name></name></author><summary type="html">Progress update I have not posted for more than a week now, and this is due to the fact that nearly all my dev time (there has been family time too, or I’ll end up with only dev time in my life - not a great prospect!) has been getting to know Agora and adapting the slightly haphazard demo material for use in my own VR app. There have been frustrating moments where things just do not work as you’d hope, but there has never really been a moment when I’ve thought “this is just never going to work”, or “if this is going to work, then it’s going to be more hassle than it’s worth!”. But I have continually been aware that it may be a laborious process, and this has been the case.</summary></entry><entry><title type="html">Multiplayer Synchronisation</title><link href="http://localhost:4000/2020/08/07/multiplayer-syncing.html" rel="alternate" type="text/html" title="Multiplayer Synchronisation" /><published>2020-08-07T02:00:00+01:00</published><updated>2020-08-07T02:00:00+01:00</updated><id>http://localhost:4000/2020/08/07/multiplayer-syncing</id><content type="html" xml:base="http://localhost:4000/2020/08/07/multiplayer-syncing.html">&lt;h1 id=&quot;using-normcore-for-two-main-objectives&quot;&gt;&lt;strong&gt;Using Normcore for two main objectives&lt;/strong&gt;:&lt;/h1&gt;

&lt;h1 id=&quot;1-syncronising-transforms&quot;&gt;1. Syncronising transforms&lt;/h1&gt;

&lt;p&gt;Normcore includes pre-built scripts that directly synchronise GameObject transforms between clients. Very simpley put, once an object has been designated to have a &lt;em&gt;‘realtime transform’&lt;/em&gt;, it will automatically be kept in sync.&lt;/p&gt;

&lt;p&gt;The same goes for players’ avatars, which allows them to interact within the shared ‘room’.&lt;/p&gt;

&lt;p&gt;Objects can be ‘owned’ by a player, allowing them to manipulate their position or other transforms. If an object is designated to have the ability to transfer ownership, then more players within the room can also interact with it.&lt;/p&gt;

&lt;p&gt;Although this is extremely simplified, it does highlight how these concepts allow rapid development of the multiplayer environment.&lt;/p&gt;

&lt;h1 id=&quot;2-syncronising-abstract-values&quot;&gt;2. Syncronising abstract values&lt;/h1&gt;

&lt;p&gt;Translating values when values change and events are raised. In this case, selecting a video and assigning it to a display screen.&lt;/p&gt;</content><author><name></name></author><summary type="html">Using Normcore for two main objectives:</summary></entry><entry><title type="html">Video Streaming Experiments</title><link href="http://localhost:4000/2020/08/07/video-streaming-experiments.html" rel="alternate" type="text/html" title="Video Streaming Experiments" /><published>2020-08-07T01:00:00+01:00</published><updated>2020-08-07T01:00:00+01:00</updated><id>http://localhost:4000/2020/08/07/video-streaming-experiments</id><content type="html" xml:base="http://localhost:4000/2020/08/07/video-streaming-experiments.html">&lt;h1 id=&quot;early-attempts-at-setting-up-a-server&quot;&gt;Early attempts at setting up a server&lt;/h1&gt;

&lt;p&gt;As can be seen here &lt;strong&gt;&lt;a href=&quot;/2020/07/27/webrtc-web-app.html&quot;&gt;Building A WebRTC Web App&lt;/a&gt;&lt;/strong&gt;, I have build a simple &lt;strong&gt;node.js&lt;/strong&gt; server using Express. I considered hosting this, or something similar - perhaps a .NET server - in AWS or Firebase. Could I still use PeerJS as a STUN/TURN server? I’m still new to this so, even though I’m picking up new stuff contiually and gradually piecing it all together, I’m uncertain how to fit it all together. I know I can learn as I go, but it’s quite a lot to get to grips with in a short space of time - there is little scope for taking the wrong route and backin up.&lt;/p&gt;

&lt;h1 id=&quot;kurento-media-server&quot;&gt;Kurento media server&lt;/h1&gt;

&lt;p&gt;I began looking at setting up a &lt;strong&gt;&lt;a href=&quot;https://www.kurento.org/whats-kurento&quot;&gt;Kurento media server&lt;/a&gt;&lt;/strong&gt;, which could be hosted as an instance in a &lt;strong&gt;&lt;a href=&quot;https://hub.docker.com/r/kurento/kurento-media-server&quot;&gt;Docker container&lt;/a&gt;&lt;/strong&gt;. This is a super idea and is enthusiastically supported by Alcwyn Parker, the Masters Degree course leader and my research tutor on this project. However, time is, again, a major issue - for both of us. We did spend some time working on sett up such a stack, but eventually it seemed sensible to work with an out-of-the-box WebRTC solution we were aware of - &lt;strong&gt;&lt;a href=&quot;https://www.agora.io/en/&quot;&gt;Agora&lt;/a&gt;&lt;/strong&gt; - as long as research showed it to have the right potential.&lt;/p&gt;

&lt;h1 id=&quot;agora&quot;&gt;Agora&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Uses WebRTC&lt;/li&gt;
  &lt;li&gt;First 10,000 minutes free every month (so likely to remain free for the kind of use we’ll be putting it to)&lt;/li&gt;
  &lt;li&gt;Pay As You Go &lt;a href=&quot;https://www.agora.io/en/pricing/&quot;&gt;pricing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Cheap&lt;/li&gt;
  &lt;li&gt;Simple to set up&lt;/li&gt;
  &lt;li&gt;Can stream voice and video&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Unity asset contains a couple of sample scenes that are fairly easy to set up. I had some problems building the scene in order to spin up multiple instances - necessary so that multiple connections can be made - as the documentation is not 100% idiot proof, but with a bit of &lt;strong&gt;tenacity&lt;/strong&gt; and patience I got something very promising working.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video style=&quot;width:720px;&quot; autoplay=&quot;&quot; loop=&quot;&quot;&gt;
    &lt;source src=&quot;\media\agora-test.mp4&quot; type=&quot;video/mp4&quot; /&gt;
    Woops! Your browser does not support the HTML5 video tag.
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;The Unity asset is for ‘live interactive video streaming’. This is geared towards one single main video channel at a time streamed to multiple locations. There is another option for ‘democratic’ open channels where all can communicate at the same time to all others. I do not this the latter option will be necessary for us, because we will be streaming single performers into the Unity space.&lt;/p&gt;

&lt;p&gt;It was great to see the video images being displayed directly onto textures in Unity, such as the cube - this should prove useful and time-saving.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Agora&lt;/strong&gt; looks to be the way to go, at least for the time being. I would prefer to be building my own server and exploring the use and potential of some exciting new technologies in a more ‘raw’ form - partly for my own personal curiosity and learning, but also for an increased self-contained, stand-alone stack that does not rely so much on external products. Perhaps this could be a later development of this project once the initial objectives have been met. For now, the precious resource of time is a prime consideration.&lt;/p&gt;</content><author><name></name></author><summary type="html">Early attempts at setting up a server</summary></entry><entry><title type="html">A Little More Light Falling On WebRTC</title><link href="http://localhost:4000/2020/07/28/webrtc-2.html" rel="alternate" type="text/html" title="A Little More Light Falling On WebRTC" /><published>2020-07-28T01:00:00+01:00</published><updated>2020-07-28T01:00:00+01:00</updated><id>http://localhost:4000/2020/07/28/webrtc-2</id><content type="html" xml:base="http://localhost:4000/2020/07/28/webrtc-2.html">&lt;h1 id=&quot;jargon-buster&quot;&gt;Jargon-buster&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;NAT: Network Address Translation&lt;/li&gt;
  &lt;li&gt;SDP: Session Description Protocol&lt;/li&gt;
  &lt;li&gt;STUN: Session Traversal Utilities for NAT&lt;/li&gt;
  &lt;li&gt;TURN: Traversal Using Relay around NAT&lt;/li&gt;
  &lt;li&gt;ICE: Interactive Connectivity Establishment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As I’m new to this whole area of sockets and media streaming, it has been a quite a minefield trying to find my way around a territory that appears to offer a lot of rich resources but it tantalisingly sparcely mapped out by those who have explored it before I.&lt;/p&gt;

&lt;p&gt;So it is quite refreshing when coming across moments of lucidity from folks who clearly knows what they are talking about.  They may have been cojoled into a paragraph or two encouraging them to unearth some of the experiencial matter that has accumulated and sedimented within their minds during the time they have been working with this material, but it’s appreciated nonetheless!&lt;/p&gt;

&lt;p&gt;Enter &lt;strong&gt;&lt;a href=&quot;https://forum.unity.com/threads/unitypeerjs-simple-webrtc-support-for-unity-webgl.310166/&quot;&gt;tiggus&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Why WebRTC instead of just normal direct tcp or udp between peers?&lt;/strong&gt;
WebRTC can be initiated from a web browser specifically thus useful for HTML5. The only sockets that you can initiate from HTML5 apps are plain http, websockets(upgraded http), and WebRTC. Vanilla TCP and UDP is not an option from the browser unless you’re using plugins. You also cannot start a WebSocket listener in a browser, only outbound.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Does it provide any Nat traversal benefits?&lt;/strong&gt;
It utilizes &lt;strong&gt;STUN&lt;/strong&gt; and &lt;strong&gt;TURN&lt;/strong&gt; for &lt;strong&gt;NAT&lt;/strong&gt; traversal so you need to provide or use someone elses STUN server and also provide some sort of signalling server to do the matchmaking, but after the two peers are connected all data flows peer to peer. The bandwidth/servers required to get STUN information is provided for free by a number of providers, STUN basically just tells you your public ip address and port so you can inform the other peer who to connect to. TURN is a relay server like you would typically do with something like Photon for instance, shuttling data through a middleman server, but is optional for clients that can’t connect with plain STUN/ICE.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Next it’s &lt;strong&gt;&lt;a href=&quot;https://stackoverflow.com/questions/59484802/ice-vs-stun-vs-turn&quot;&gt;hobbs&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;TURN&lt;/strong&gt; is a relay — both clients send data to the TURN server, which forwards it to the other client.
&lt;strong&gt;STUN&lt;/strong&gt; is not a relay — the STUN server helps to “make the connection” between the clients (by discovering and exchanging their external host:port pairs), after which they send data to each other directly. However, STUN doesn’t work with all NAT/firewall setups, so TURN is used when STUN fails.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;And from &lt;strong&gt;&lt;a href=&quot;https://www.twilio.com/docs/stun-turn/faq&quot;&gt;twilio.com&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;STUN&lt;/strong&gt;, &lt;strong&gt;TURN&lt;/strong&gt;, and &lt;strong&gt;ICE&lt;/strong&gt; are a set of IETF standard protocols for negotiating traversing NATs when establishing peer-to-peer communication sessions. WebRTC and other VoIP stacks implement support for ICE to improve the reliability of IP communications.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;A host uses &lt;strong&gt;Session Traversal Utilities for NAT&lt;/strong&gt; (STUN) to discover its public IP address when it is located behind a NAT/Firewall. When this host wants to receive an incoming connection from another party, it provides this public IP address as a possible location where it can receive a connection. If the NAT/Firewall still won’t allow the two hosts to connect directly, they make a connection to a server implementing Traversal Using Relay around NAT (TURN), which will relay media between the two parties.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Interactive Connectivity Establishment&lt;/strong&gt; (ICE) is a blanket standard that describes how to coordinate STUN and TURN to make a connection between hosts. Twilio’s Network Traversal Service implements STUN and TURN for ICE-compatible clients, such as browsers supporting the WebRTC standard.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thank you &lt;strong&gt;tiggus&lt;/strong&gt;, &lt;strong&gt;hobbs&lt;/strong&gt; and &lt;strong&gt;twilio.com&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://medium.com/av-transcode/what-is-webrtc-and-how-to-setup-stun-turn-server-for-webrtc-communication-63314728b9d0&quot;&gt;Medium: What is WebRTC and How to Setup STUN/TURN Server for WebRTC Communication?&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;ICE&lt;/strong&gt; (Interactive Connectivity Establishment) is a protocol used to generate media traversal candidates that can be used in WebRTC applications, and it can be successfully sent and received through Network Address Translation (NAT)s using STUN and TURN.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;STUN&lt;/strong&gt; (Session Traversal Utilities for NAT) allows applications to discover the presence and types of NATs and firewalls between them and on the public Internet. It can be used by any device to determine the IP address and port allocated to it by a NAT.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;TURN&lt;/strong&gt; (Traversal Using Relays around NAT) Server allows clients to send and receive data through an intermediary server. The TURN protocol is the extension to STUN.&lt;br /&gt;
TURN is most useful for Web, Mobile and IoT clients on networks masqueraded by symmetric NAT devices. But the TURN server cost is high because of the server utilization and huge bandwidth usage in the case where more client connections are established.&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name></name></author><summary type="html">Jargon-buster</summary></entry></feed>